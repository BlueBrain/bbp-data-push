"""
Create a 'VolumetricDataLayer' resource payload to push into Nexus. This script has been designed 
to function with volumetric files generated by the Atlas pipeline.
(link to Atlas pipeline confluence documentation)
"""

import os
import yaml
import numpy as np
import nrrd
import fnmatch
from kgforge.core import Resource 

from bba_dataset_push.commons import (getBrainRegionName, getHierarchyContent, 
                                      getVoxelType, addContribution)
from bba_dataset_push.logging import createLogHandler

L = createLogHandler(__name__, "./push_volumetric.log")

def createNrrdResources(forge, inputpath, voxels_resolution, config_path, input_hierarchy):
    
    ## Constructs the payloads schema according to the 3 different possible volumetric files to be pushed
    config_file = open(config_path)
    config_content = yaml.safe_load(config_file.read().strip())
    config_file.close()
    volumetric_path = None
    try:
        volumetric_path = config_content["GeneratedDatasetPath"]["VolumetricFile"]
    except KeyError as error:
        L.error(f'KeyError: {error}. The key ["GeneratedDatasetPath"]["VolumetricFile"] is not '\
                'found in the push_dataset_config file.')
        exit(1)
            
    resource_type = 'VolumetricDataLayer'
    for filepath in inputpath:
        flat_tree = {}
        hierarchy_tag = None
        isFolder = False
        if os.path.isdir(filepath):
            isFolder = True
            if (filepath == volumetric_path["cell_densities"] or 
                filepath == volumetric_path["neuron_densities"]):
                directory = filepath
                files = os.listdir(directory)
                pattern = '*_density.nrrd'
                files_nrrd = fnmatch.filter(files, pattern)
                if not files_nrrd:
                    L.error(f"Error: '{filepath}' do not contain any cell density volumetric files.")
                    exit(1)
                
                # this is going ot be the "name" of the resource
                filepath = os.path.join(directory, files_nrrd[0])
                filename_noext = os.path.splitext(os.path.basename(filepath))[0]
                file_extension = os.path.splitext(os.path.basename(filepath))[1][1:]
                hierarchy_tag = "hierarchy"
                voxel_type = 'intensity'
                resource_types = [resource_type, 'GliaCellDensity', 'CellDensityDataLayer']
            else:
                L.error(f"Error: The '{filepath}' folder do not correspond to one of the cell density "\
                      "folder dataset defined in the VolumetricFile section of the generated dataset "\
                      "configuration file.")
                exit(1)
            description = f"Cell density volume for {filename_noext.replace('_', ' ')} at a resolution "\
                f"of {voxels_resolution} microns. Output of the cell density pipeline by Csaba Eroe."
        else:
            if filepath == volumetric_path["annotation_hybrid"]:
                hierarchy_tag = "hierarchy"
                voxel_type = 'label'
                resource_types = [resource_type, 'BrainParcellationDataLayer']
                description = f"Hybrid annotation volume from ccfv2 and ccfv3 at {voxels_resolution} "\
                    f"microns. The version replaces the leaf regions in ccfv3 with the leaf region of "\
                    "ccfv2, which have additional levels of hierarchy."
            elif filepath == volumetric_path["annotation_l23split"]:
                hierarchy_tag = "hierarchy_l23split"
                voxel_type = 'label'
                resource_types = [resource_type, 'BrainParcellationDataLayer']
                description = f"Hybrid annotation volume from ccfv2 and ccfv3 at {voxels_resolution} "\
                    f"microns, also includes the split of layer 2 and layer 3."
            else:
                L.error(f"Error: '{filename_noext}' name do not correspond to one of the annotation "\
                        "file dataset defined in the VolumetricFile section of the generated dataset "\
                        "configuration file.")
                exit(1)
            
            # this is going ot be the "name" of the resource
            filename_noext = os.path.splitext(os.path.basename(filepath))[0]
            file_extension = os.path.splitext(os.path.basename(filepath))[1][1:]
            
        atlas_reference_system_id = "https://bbp.epfl.ch/neurosciencegraph/data/allen_ccfv3_spatial_reference_system"
        id_atlas_release = 'https://bbp.epfl.ch/neurosciencegraph/data/e2e500ec-fe7e-4888-88b9-b72425315dda'
        region_id = 997 #default: 997 --> root, whole brain
    
    # Add the link to the spatial ref system
        isRegisteredIn = {
            "@type": ["BrainAtlasSpatialReferenceSystem","AtlasSpatialReferenceSystem"],
            "@id": atlas_reference_system_id
        }
        try: 
            hierarchy_path = getHierarchyContent(input_hierarchy, config_content, hierarchy_tag)
        except KeyError as e:
            L.error(f"KeyError: {e}")
            exit(1)
        
        try:
            region_name, hierarchy_tree = getBrainRegionName(region_id, hierarchy_path, flat_tree)
        except KeyError as e:
            L.error(f"KeyError: {e}")
            exit(1)
            
        #flat_tree = hierarchy_tree
        brainLocation = {
            "brainRegion": {
                "@id": f"mba:{region_id}",
                "label": region_name
            },
    
            "atlasSpatialReferenceSystem": {
                "@type": ["BrainAtlasSpatialReferenceSystem","AtlasSpatialReferenceSystem"],
                "@id": atlas_reference_system_id
            }
        }
        
        # Parsing the header of the NRRD file
        header = None
        try:
            header = nrrd.read_header(filepath)
        except nrrd.errors.NRRDError as e:
            L.error(f"NrrdError: {e}")
            L.info("Aborting pushing process.") #setLevel(logging.INFO)
            exit(1)
        
        #Constants
        default_sampling_period = 30
        default_sampling_time_unit = 'ms'
        spatial_unit = 'Âµm'
        
        config = {
            "file_extension": file_extension,
            "sampling_space_unit": spatial_unit,
            "sampling_period": default_sampling_period,
            "sampling_time_unit": default_sampling_time_unit
        }
        
        # We create a 1st payload that will be recycled in case of multiple files to push
        distribution_file = forge.attach(filepath) 
        nrrd_resource = Resource(
            type = resource_types, 
            name = filename_noext.replace("_", " ").title(), 
            distribution = distribution_file,
            description = description,
            isRegisteredIn = isRegisteredIn,
            brainLocation = brainLocation,
            atlasRelease = {"@id": id_atlas_release}
            )
        
        nrrd_resource = addNrrdProps(nrrd_resource, header, config, voxel_type)
        try:
            nrrd_resource.contribution = addContribution(forge, nrrd_resource)
        except ValueError as e:
            L.error(f"ValueError: {e}")
            exit(1)
        dataset = [nrrd_resource]
        
        # If the input is a folder containing the cell density file to push
        if isFolder:
            for f in range(1,len(files_nrrd)): #start at the 2nd file
                
                dataset = [nrrd_resource]
                filepath = os.path.join(directory, files_nrrd[f])
                filename_noext = os.path.splitext(os.path.basename(filepath))[0]
                distribution_file = forge.attach(filepath) 
                #Use forge.reshape instead ?
                nrrd_resources = Resource(
                    type = nrrd_resource.type, 
                    name = filename_noext.replace("_", " ").title(), 
                    distribution = distribution_file,
                    description = nrrd_resource.description,
                    contribution = nrrd_resource.contribution,
                    isRegisteredIn = nrrd_resource.isRegisteredIn,
                    brainLocation = nrrd_resource.brainLocation,
                    atlasRelease = nrrd_resource.atlasRelease,
                    componentEncoding = nrrd_resource.componentEncoding,
                    fileExtension = nrrd_resource.fileExtension,
                    dimension = nrrd_resource.dimension,
                    sampleType = nrrd_resource.sampleType,
                    worldMatrix = nrrd_resource.worldMatrix,
                    resolution = nrrd_resource.resolution
                    )
                
                dataset.append(nrrd_resources)
                
    return dataset


def addNrrdProps(resource, nrrd_header, config, voxel_type):
    """
    Add to the resource all the fields expected for a VolumetricDataLayer/NdRasterthat can be 
    found in the NRRD header. A resource dictionary must exist and be provided (even if empty).
    """
    
    NRRD_TYPES_TO_NUMPY = {
    "signed char": "int8",
    "int8": "int8",
    "int8_t": "int8",
    "uchar": "uint8",
    "unsigned char": "uint8",
    "uint8": "uint8",
    "uint8_t": "uint8",
    "short": "int16",
    "short int": "int16",
    "signed short": "int16",
    "signed short int": "int16",
    "int16": "int16",
    "int16_t": "int16",
    "ushort": "int16",
    "unsigned short": "uint16",
    "unsigned short int": "uint16",
    "uint16": "uint16",
    "uint16_t": "uint16",
    "int": "int32",
    "signed int": "int32",
    "int32": "int32",
    "int32_t": "int32",
    "uint": "uint32",
    "unsigned int": "uint32",
    "uint32": "uint32",
    "uint32_t": "uint32",
    "longlong": "int64",
    "long long": "int64",
    "long long int": "int64",
    "signed long long": "int64",
    "signed long long int": "int64",
    "int64": "int64",
    "int64_t": "int64",
    "ulonglong": "uint64",
    "unsigned long long": "uint64",
    "unsigned long long int": "uint64",
    "uint64": "uint64",
    "uint64_t": "uint64",
    "float": "float32",
    "double": "float64"
    }
    
    space_origin = None
    if "space origin" in nrrd_header:
        space_origin = nrrd_header["space origin"].tolist()
    else:
        if nrrd_header["dimension"] == 2:
            space_origin = [0.0, 0.0]
        elif nrrd_header["dimension"] == 3:
            space_origin = [0.0, 0.0, 0.0]

    space_directions = None
    if "space directions" in nrrd_header:
        # replace the nan that pynrrd adds to None (just like in NRRD spec)
        space_directions = []
        for col in nrrd_header["space directions"].tolist():
            if np.isnan(col).any():
                space_directions.append(None)
            else:
                space_directions.append(col)

    # Here, 'space directions' being missing in the file, we hardcode an identity matrix.
    # If we have 4 dimensions, we say
    else:
        if nrrd_header["dimension"] == 2:
            space_directions = [[1, 0], [0, 1]]
        elif nrrd_header["dimension"] == 3:
            space_directions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
        elif nrrd_header["dimension"] == 4:
            # the following is a very lousy way to determine if among the 4 dims,
            # or the first is components or the last is time...
            if nrrd_header["sizes"][0] < (np.mean(nrrd_header["sizes"] * 0.20)):
                space_directions = [None, [1, 0, 0], [0, 1, 0], [0, 0, 1]] # component
            else:
                space_directions = [[1, 0, 0], [0, 1, 0], [0, 0, 1], None] # time

        elif nrrd_header["dimension"] == 5:
            space_directions = [None, [1, 0, 0], [0, 1, 0], [0, 0, 1], None]

    resource.componentEncoding = NRRD_TYPES_TO_NUMPY[nrrd_header["type"]]
    resource.endianness = nrrd_header["endian"]
    resource.bufferEncoding = nrrd_header["encoding"]
    resource.fileExtension = config["file_extension"]
    resource.dimension = []

    component_dim_index = -1
    passed_spatial_dim = False
    # for each dimension
    for i in range(0, nrrd_header["dimension"]):
        current_dim = {}
        current_dim["size"] = nrrd_header["sizes"][i].item()


        # this is a spatial dim
        if space_directions[i]:
            passed_spatial_dim = True
            current_dim["@type"] = "SpaceDimension"
            current_dim["unitCode"] = config["sampling_space_unit"]

        # this can be a component or a time dim
        else:
            # this is a time dim (because after space dim)
            if passed_spatial_dim:
                current_dim["@type"] = "nsg:TimeDimension"
                current_dim["samplingPeriod"] = config["sampling_period"]
                current_dim["unitCode"] = config["sampling_time_unit"]

            # this is a component dim (because before space dim)
            else:
                # decide of the label

                component_dim_index = i
                current_dim["@type"] = "ComponentDimension"
                # current_dim["name"] = default_sample_type_multiple_components if current_dim["size"] > 1 else default_sample_type_single_component
                try:
                    current_dim["name"] = getVoxelType(current_dim["size"])
                except ValueError as e:
                    L.error(f"ValueError: {e}")
                    exit(1)      

        resource.dimension.append(current_dim)

    # repeating the name of the component dimension in the "sampleType" base level prop
    if component_dim_index >= 0:
        resource.sampleType = resource.dimension.component_dim_index.name

    # As no component dim was mentioned in metadata, it means the component is of size 1
    else:
        # prepend a dimension component
        try:
            name = getVoxelType(voxel_type, 1)
        except ValueError as e:
            L.error(f"ValueError: {e}")
            exit(1)     
        component_dim = {
            "@type": "ComponentDimension",
            "size": 1,
            "name": name
        }
        resource.dimension.insert(0, component_dim)

        resource.sampleType = component_dim["name"]

    # creating the world matrix (column major)
    # 1. pynrrd creates a [nan, nan, nan] line for each 'space directions' that is 'none' in the header.
    # We have to strip them off.
    worldMatrix = None
    r = [] # rotation mat
    o = space_origin
    for col in space_directions:
        if col != None:
            r.append(col)

    # if 3D, we create a 4x4 homogeneous transformation matrix
    if len(r) == 3:
        worldMatrix = [
            r[0][0], r[0][1], r[0][2], 0,
            r[1][0], r[1][1], r[1][2], 0,
            r[2][0], r[2][1], r[2][2], 0,
            o[0], o[1], o[2], 1
        ]

    # if 2D, we create a 3x3 homogeneous transformation matrix
    if len(r) == 2:
        worldMatrix = [
            r[0][0], r[0][1], 0,
            r[1][0], r[1][1], 0,
            o[0], o[1], 1
        ]

    # nesting the matrix values into object with @value props
    for i in range(0, len(worldMatrix)):
        # worldMatrix[i] = {"@value": float(worldMatrix[i])}
        worldMatrix[i] = float(worldMatrix[i])

    resource.worldMatrix = worldMatrix


    resource.resolution = {
        "value": r[0][0],
        "unitCode": config["sampling_space_unit"]
    }
    
    return resource