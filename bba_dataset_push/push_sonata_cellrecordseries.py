"""
Create a 'CellRecordSeries' resource payload to push into Nexus. This script has been designed 
to function with sonata h5 files storing 3D brain cell positions and orientations and 
generated by the Atlas pipeline.
To know more about 'Mesh' resources and Nexus, see https://bluebrainnexus.io.
Link to BBP Atlas pipeline confluence documentation: https://bbpteam.epfl.ch/project/spaces/x/rS22Ag
"""

import os
import yaml
import h5py
from kgforge.core import Resource
from kgforge.specializations.stores.demo_store import DemoStore

from bba_dataset_push.commons import add_contribution, append_provenance_to_description
from bba_dataset_push.logging import create_log_handler

L = create_log_handler(__name__, "./push_cellrecord.log")


def create_cell_record_resources(
    forge, inputpath: list, voxels_resolution: int, config_path, provenances: list, verbose
) -> list:
    """
    Construct the input sonata hdf5 dataset property payloads that will be push with the
    corresponding files into Nexus as a resource.

    Parameters:
        forge : instantiated and configured forge object.
        inputpath : input datasets paths. These datasets are either volumetric files or folder
                    containing volumetric files.
        voxels_resolution : voxel resolution value.
        config_path : configuration yaml file path containing the names and paths of the
                      atlas-pipeline generated datasets.
        provenances : string name of the module that generated input datasets.

    Returns:
        dataset : list containing as much Resource object as input datasets. Each Resource is
        defined by an attached input file and its properties described in a payload.
    """
    L.setLevel(verbose)

    ## Constructs the payloads schema according to the 2 different possible mesh dataset to be pushed
    config_file = open(config_path)
    config_content = yaml.safe_load(config_file.read().strip())
    config_file.close()
    try:
        sonata_path = config_content["GeneratedDatasetPath"]["CellPositionFile"]
    except KeyError as error:
        L.error(
            f'KeyError: {error}. The key ["GeneratedDatasetPath"]["CellPositionFile"] is "\
                "not found in the input datasets configuration file'
        )
        exit(1)
    # Constructs the Resource properties payloads accordingly to the input atlas Mesh datasets
    dataset = []
    Measures_table = {
        "x": "Cell position along the X axis",
        "y": "Cell position along the Y axis",
        "z": "Cell position along the Z axis",
        "orientation_w": "Component w of the cell orientation quaternion",
        "orientation_x": "Component x of the cell orientation quaternion",
        "orientation_y": "Component y of the cell orientation quaternion",
        "orientation_z": "Component z of the cell orientation quaternion",
        "cell_type": "Label of the cell type",
        "region_id": "Region identifiers (AIBS Structure IDs)",
    }

    # Constants
    module_prov = "positions-and-orientations"
    spatial_unit = "Âµm"
    atlas_reference_system_id = (
        "https://bbp.epfl.ch/neurosciencegraph/data/allen_ccfv3_spatial_reference_system"
    )
    id_atlas_release = (
        "https://bbp.epfl.ch/neurosciencegraph/data/e2e500ec-fe7e-4888-88b9-b72425315dda"
    )
    region_id = 997  # default: 997 --> root, whole brain
    region_name = "root"
    # Link to the spatial ref system
    isRegisteredIn = {
        "@type": ["BrainAtlasSpatialReferenceSystem", "AtlasSpatialReferenceSystem"],
        "@id": atlas_reference_system_id,
    }

    brainLocation = {
        "brainRegion": {"@id": f"mba:{region_id}", "label": region_name},
        "atlasSpatialReferenceSystem": {
            "@type": ["BrainAtlasSpatialReferenceSystem", "AtlasSpatialReferenceSystem"],
            "@id": atlas_reference_system_id,
        },
    }
    # If multiple files and multiple Atlas
    for filepath in inputpath:
        try:
            if os.path.samefile(filepath, sonata_path["cell_records_sonata"]):
                if filepath.endswith(".h5"):
                    atlas_description = "Mouse ccfv2-ccfv3 Hybrid annotation volume"
                else:
                    L.error(
                        f"Error: cell-record sonata dataset '{filepath}' is not a sonata "
                        ".h5 file"
                    )
                    exit(1)
            else:
                L.error(
                    f"Error: The '{filepath}' folder do not correspond to a Sonata .h5 file "
                    "dataset defined in the CellPositionFile section of the input datasets "
                    "configuration file"
                )
                exit(1)
        except FileNotFoundError as e:
            L.error(f"FileNotFoundError: {e}")
            exit(1)

        # We create a 1st payload which will serve as template for the others
        filename_noext = os.path.splitext(os.path.basename(filepath))[0]
        # file_extension = os.path.splitext(os.path.basename(filepath))[1][1:] if needed

        # We create a 1st payload that will be recycled in case of multiple files to push

        description = (
            f"Sonata .h5 file storing the 3D cell positions and orientations of the "
            f"{atlas_description} (spatial resolution of {voxels_resolution} "
            f"{spatial_unit})."
        )

        if provenances[0]:
            try:
                prov_description = append_provenance_to_description(provenances, module_prov)
                description = f"{description} {prov_description}"
            except ValueError as e:
                L.error(f"Value Error in provenance content. {e}")
                exit(1)
        try:
            cell_collections = h5py.File(filepath, "r")
        except OSError as e:
            L.error(f"OSError when trying to open the input file {filepath}. {e}")
            L.info("Aborting pushing process.")  # setLevel(logging.INFO)
            exit(1)

        recordMeasure = []
        try:
            Datasets = cell_collections["nodes"]["atlas_cells"]["0"]
            for Dataset in Datasets.keys():
                if Dataset in Measures_table:
                    Measure_payload = {
                        "@type": "RecordMeasure",
                        "description": Measures_table[Dataset],
                        "componentEncoding": f"{Datasets[Dataset].dtype}",
                        "name": f"{Dataset}",
                    }
                    if Dataset == "cell_type":
                        cell_types = Datasets["@library"]["cell_type"]
                        if all(isinstance(x, bytes) for x in cell_types):
                            cell_types = [s.decode("UTF-8") for s in cell_types]
                        elif any(isinstance(x, bytes) for x in cell_types):
                            L.error(
                                "ValueError: @library/cell_type contains string and bytes "
                                "(literal string). The content need to be uniform."
                            )
                            exit(1)
                        Measure_payload["label"] = {
                            f"{i}": cell_types[i] for i in range(0, len(cell_types))
                        }
                    recordMeasure.append(Measure_payload)
        except KeyError as e:
            L.error(
                f"KeyError during the information extraction of the dataset in the input "
                f"file {filepath}. {e}"
            )
            exit(1)

        try:
            numberOfRecords = {
                "@type": "xsd:long",
                "@value": cell_collections.get("/nodes/atlas_cells/0/x").shape[0],
            }
        except KeyError as e:
            L.error(
                f"KeyError during the information extraction of the dataset in the input "
                f"file {filepath}. {e}"
            )
            exit(1)

        # add personalised content_type = "application/" + extension (according to mime convention)
        distribution_file = forge.attach(filepath)
        cellrecord_resource = Resource(
            type="CellRecordSeries",
            name=filename_noext.replace("_", " ").title(),
            description=description,
            atlasRelease={"@id": id_atlas_release},
            isRegisteredIn=isRegisteredIn,
            brainLocation=brainLocation,
            distribution=distribution_file,
            recordMeasure=recordMeasure,
            numberOfRecords=numberOfRecords,
            bufferEncoding="binary",
        )
        # resource.fileExtension = config["file_extension"]

        if isinstance(forge._store, DemoStore):
            cellrecord_resource.contribution = []
        else:
            try:
                cellrecord_resource.contribution, log_info = add_contribution(
                    forge, cellrecord_resource
                )
                L.info("\n".join(log_info))
            except Exception as e:
                L.error(f"Error: {e}")
                exit(1)

        dataset.append(cellrecord_resource)

    return dataset
