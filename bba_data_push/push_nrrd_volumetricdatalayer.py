"""
Create a 'VolumetricDataLayer' resource payload to push into Nexus. This script has 
been designed to function with volumetric files generated by the Atlas pipeline.
To know more about 'VolumetricDataLayer' resources and Nexus, see 
https://bluebrainnexus.io.
Link to BBP Atlas pipeline confluence documentation: 
https://bbpteam.epfl.ch/project/spaces/x/rS22Ag
"""

import os
import yaml
import numpy as np
import nrrd
import fnmatch
import re
from datetime import datetime
from uuid import uuid4
from kgforge.core import Resource
from kgforge.specializations.stores.demo_store import DemoStore

from bba_data_push.commons import (
    get_voxel_type,
    add_contribution,
    append_provenance_to_description,
)
from bba_data_push.logging import create_log_handler

L = create_log_handler(__name__, "./push_volumetric.log")


def create_volumetric_resources(
    forge,
    inputpath: list,
    voxels_resolution: int,
    config_path,
    provenances: list,
    new_atlasrelease_hierarchy_path,
    verbose,
) -> list:
    """
    Construct the input volumetric dataset property payloads that will be push with
    the corresponding files into Nexus as a resource.

    Parameters:
        forge : instantiated and configured forge object.
        inputpath : input datasets paths. These datasets are either volumetric files
                    or folder containing volumetric files.
        voxels_resolution : voxel resolution value.
        config_path : configuration yaml file path containing the names and paths of
                      the atlas-pipeline generated datasets.
        provenances : string name of the module that generated input datasets.

    Returns:
        dataset : list containing as much Resource object as input datasets. Each
        Resource is defined by an attached input file and its properties described in
        a payload.
    """
    L.setLevel(verbose)

    config_file = open(config_path)
    config_content = yaml.safe_load(config_file.read().strip())
    config_file.close()
    try:
        volumes = config_content["GeneratedDatasetPath"]["VolumetricFile"]
    except KeyError as error:
        L.error(f"KeyError: {error} is not found in the dataset configuration file.")
        exit(1)

    # For a new atlas release creation verify that the right parcellation volume and
    # hierarchy file have been provided
    hierarchy_found = False
    parcellation_found = []
    if new_atlasrelease_hierarchy_path:
        try:
            for hierarchy_dataset in config_content["HierarchyJson"]:
                if os.path.samefile(
                    new_atlasrelease_hierarchy_path,
                    config_content["HierarchyJson"][hierarchy_dataset],
                ):
                    hierarchy_found = True
                    break
            if not hierarchy_found:
                L.error(
                    "Error: The hierarchy file provided with the argument "
                    "'new-atlasrelease-hierarchy-path' does not correspond to one of "
                    "the hierarchy files listed in the dataset configuration file."
                )
                exit(1)
            for inputdata in inputpath:
                if os.path.samefile(volumes["annotation_hybrid_l23split"], inputdata):
                    parcellation_found.append("annotation_hybrid_l23split")
                if os.path.samefile(
                    volumes["annotation_realigned_l23split"], inputdata
                ):
                    parcellation_found.append("annotation_realigned_l23split")
            if not parcellation_found:
                L.error(
                    "Error: The argument 'new-atlasrelease-hierarchy-path' has been "
                    "provided but no parcellation volume corresponding to an atlas "
                    "Release (aka 'annotation_hybrid_l23split' or "
                    "'annotation_realigned_l23split' in the dataset configuration "
                    "file) has been found among the input datasets."
                )
                exit(1)
        except FileNotFoundError as error:
            L.error(
                f"Error: {error}. The argument 'new-atlasrelease-hierarchy-path' need "
                "to be provided with the right hierarchy file and right parcellation "
                "volume that are listed in the dataset configuration file."
            )

    # Mutual resource properties

    atlas_reference_system_id = (
        "https://bbp.epfl.ch/neurosciencegraph/data/"
        "allen_ccfv3_spatial_reference_system"
    )

    region_id = 997  # default: 997 --> root, whole brain
    region_name = "root"

    # Link to the spatial ref system
    isRegisteredIn = {
        "@type": ["BrainAtlasSpatialReferenceSystem", "AtlasSpatialReferenceSystem"],
        "@id": atlas_reference_system_id,
    }

    brainLocation = {
        "brainRegion": {"@id": f"mba:{region_id}", "label": region_name},
        "atlasSpatialReferenceSystem": {
            "@type": [
                "BrainAtlasSpatialReferenceSystem",
                "AtlasSpatialReferenceSystem",
            ],
            "@id": atlas_reference_system_id,
        },
    }

    subject = {
        "@type": "Subject",
        "species": {
            "@id": "http://purl.obolibrary.org/obo/NCBITaxon_10090",
            "label": "Mus musculus",
        },
    }

    # Create contribution
    if isinstance(forge._store, DemoStore):
        contribution = []
    else:
        try:
            contribution, log_info = add_contribution(forge)
            L.info("\n".join(log_info))
        except Exception as e:
            L.error(f"Error: {e}")
            exit(1)

    # Generation property:
    # "atTime": "{date time}",
    # generation = {
    #     "@type": "Generation",
    #     "activity": {"@id": "ggg", "@type": ["Activity", "nsg:pipelinerule"]},
    # }

    # Config constants
    default_sampling_period = 30
    default_sampling_time_unit = "ms"
    spatial_unit = "µm"

    # Resource payload properties values corresponding to the different possible Atlas
    # volumetric datasets
    resource_type = "VolumetricDataLayer"
    description_hybrid = (
        f"Hybrid annotation volume from ccfv2 and ccfv3 at {voxels_resolution} "
        "microns"
    )
    description_hybrid_split = (
        f"{description_hybrid} with the isocortex layer 2 and 3 split"
    )
    description_realigned = (
        "Annotation volume corresponding to the AIBS volume CCFv2 realigned to the "
        f"AIBS volume CCFv3 at {voxels_resolution} microns"
    )
    description_realigned_simple = (
        f"Realigned annotation volume from ccfv2 to ccfv3 at {voxels_resolution} "
        "microns"
    )
    description_realigned_split = (
        f"{description_realigned} with the isocortex layer 2 and 3 split"
    )
    description_realigned_split_simple = (
        f"{description_realigned_simple} with the isocortex layer 2 and 3 split"
    )
    description_orientation = "Quaternions field (w,x,y,z) defined over the"
    description_orientation_end = (
        f"(spatial resolution of {voxels_resolution} µm) and representing the neuron "
        "axone-to-dendrites orientation to voxels from the Isocortex region."
    )
    description_orientation_hybrid = (
        f"{description_orientation} CCF v2-v3 Hybrid annotation volume "
        f"{description_orientation_end}"
    )
    description_orientation_realigned = (
        f"{description_orientation} CCF v2-to-v3 Realigned annotation volume "
        f"{description_orientation_end}"
    )
    description_PH = (
        "The layers are ordered with respect to depth, which means that the layer "
        "which is the closest from the skull is the first layer (upper layer) and the "
        "deepest one is the last (lower layer)."
    )
    description_PH_hybrid_split = (
        "Placement hints (cortical distance of voxels to layer boundaries) of the "
        f"Isocortex Layer XX of the {description_hybrid_split}. {description_PH}"
    )
    description_PH_realigned_split = (
        "Placement hints (cortical distance of voxels to layer boundaries) of the "
        f"Isocortex Layer XX of the {description_realigned_split_simple} with the "
        f"isocortex layer 2 and 3 split. {description_PH}"
    )

    #  "@type": ["VolumetricDataLayer", "BrainParcellationDataLayer"],
    derivation_ccfv2v3 = [
        {
            "@type": "Derivation",
            "description": "The ccfv3 (2017) has smoother region borders, without "
            "jaggies. "
            "The enveloppe or most regions was used in this volume",
            "entity": {
                "@id": "https://bbp.epfl.ch/neurosciencegraph/data/"
                "025eef5f-2a9a-4119-b53f-338452c72f2a",
                "@type": "Dataset",
            },
        },
        {
            "@type": "Derivation",
            "description": "The ccfv2 (2011) has a finer granularity than ccfv3 in "
            "term of leaf nodes, these were imported in this volume",
            "entity": {
                "@id": "https://bbp.epfl.ch/neurosciencegraph/data/"
                "7b4b36ad-911c-4758-8686-2bf7943e10fb",
                "@type": "Dataset",
            },
        },
    ]
    # "@type": ["VolumetricDataLayer", "BrainParcellationDataLayer"],
    derivation_hybrid = {
        "@type": "Derivation",
        "entity": {
            "@id": "https://bbp.epfl.ch/neurosciencegraph/data/"
            "7b2f498d-a20f-4992-8410-d8b44ec72c9a",
            "@type": "Dataset",
        },
    }
    derivation_hybrid_described = derivation_hybrid.copy()
    derivation_hybrid_described[
        "description"
    ] = "The separation between layer 2 and layer 3 was performed on the source volume."

    derivation_realigned = {
        "@type": "Derivation",
        "description": "AIBS volume CCFv2 realigned to the AIBS volume CCFv3",
        "entity": {
            "@id": "https://bbp.epfl.ch/neurosciencegraph/data/"
            "d318062c-b24e-4eab-830c-37d2b0ec4c00",
            "@type": "Dataset",
        },
    }

    # Dictionary containing the possible volumetric dataset to push

    volumetric_data = {
        "parcellations": {
            f"{volumes['annotation_hybrid']}": [
                "combine-annotations",
                f"{description_hybrid}. The version "
                "replaces the leaf regions in ccfv3 with the leaf region of "
                "ccfv2, which have additional levels of hierarchy.",
                derivation_ccfv2v3,
                "atlasrelease_ccfv2v3",
                "parcellationId"
            ],
            f"{volumes['annotation_hybrid_l23split']}": [
                "split-isocortex-layer-23",
                description_hybrid_split,
                derivation_hybrid_described,
                "atlasrelease_hybrid",
                "parcellationId"
            ],
            f"{volumes['annotation_realigned_l23split']}": [
                "split-isocortex-layer-23",
                description_realigned_split,
                derivation_realigned,
                "atlasrelease_realigned",
                "parcellationId"
            ],
        },
        "cell_orientations": {
            f"{volumes['cell_orientations_hybrid']}": [
                "orientation-field",
                description_orientation_hybrid,
                derivation_hybrid,
                "atlasrelease_ccfv2v3",
                "quaternion"
            ],
            f"{volumes['cell_orientations_realigned']}": [
                "orientation-field",
                description_orientation_realigned,
                derivation_realigned,
                "atlasrelease_realigned",
                "quaternion"
            ],
        },
        "placement_hints": {
            f"{volumes['placement_hints_hybrid_l23split']}": [
                "placement-hints isocortex",
                description_PH_hybrid_split,
                derivation_hybrid,
                "atlasrelease_ccfv2v3",
                "distance"
            ],
            f"{volumes['placement_hints_realigned_l23split']}": [
                "placement-hints isocortex",
                description_PH_realigned_split,
                derivation_realigned,
                "atlasrelease_realigned",
                 "distance"
            ],
        },
        "cell_densities": {
            f"{volumes['cell_densities']}": [
                "glia-cell-densities",
                description_hybrid,
                derivation_hybrid,
                "atlasrelease_hybrid",
                "quantity"
            ],
            f"{volumes['neuron_densities']}": [
                "inhibitory-and-excitatory-neuron-densities",
                description_hybrid,
                derivation_hybrid,
                "atlasrelease_hybrid",
                "quantity"
            ],
        },
    }

    # Constructs the Resource properties payloads with the dictionary of properties
    datasets = []
    atlasreleases = {"atlas_releases": [], "hierarchy": []}
    atlasrelease_dict = {"atlasrelease_choice": False, "hierarchy": False}
    atlasRelease = {}
    for filepath in inputpath:
        file_found = False
        isFolder = False
        derivation = False
        isCellDensity = False
        isPH = False
        for dataset in volumetric_data["cell_densities"]:
            try:
                if os.path.samefile(filepath, dataset):
                    file_found = True
                    isCellDensity = True
                    if os.path.isdir(filepath):
                        isFolder = True
                        directory = filepath
                        files = os.listdir(directory)
                        pattern = "*_density.nrrd"
                        files_list = fnmatch.filter(files, pattern)
                        if not files_list:
                            L.error(
                                f"Error: '{filepath}' do not contain any cell density "
                                "volumetric files."
                            )
                            exit(1)
                        # this is going ot be the "name" of the resource
                        filepath = os.path.join(directory, files_list[0])
                        filename_noext = os.path.splitext(os.path.basename(filepath))[0]
                        file_extension = os.path.splitext(os.path.basename(filepath))[
                            1
                        ][1:]
                        cell_density_file = filename_noext.replace("_", " ")
                        cell_density_name = (
                            f"{cell_density_file[0].upper()}" f"{cell_density_file[1:]}"
                        )
                        voxel_type = "intensity"
                        resource_types = [
                            resource_type,
                            "GliaCellDensity",
                            "CellDensityDataLayer",
                        ]
                        description = (
                            f"{cell_density_name} volume for the "
                            f"{volumetric_data['cell_densities'][dataset][1]}."
                        )
                        module_tag = volumetric_data["cell_densities"][dataset][0]
                        derivation = volumetric_data["cell_densities"][dataset][2]
                        atlasrelease_choice = volumetric_data["cell_densities"][
                            dataset
                        ][3]
                        dataSampleModality = [volumetric_data["cell_densities"][
                            dataset
                        ][4]]
                        break
                    else:
                        L.error(
                            f"Error: cell density dataset '{filepath}' is not a folder "
                            "containing cell densities .nrrd files"
                        )
                        exit(1)
            except FileNotFoundError:
                pass

        # Jump it if a file has been recognized
        if not file_found:
            for dataset in volumetric_data["parcellations"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        if filepath.endswith(".nrrd"):
                            file_found = True
                            voxel_type = "label"
                            resource_types = [
                                resource_type,
                                "BrainParcellationDataLayer",
                            ]
                            description = (
                                f"{volumetric_data['parcellations'][dataset][1]}"
                            )
                            module_tag = volumetric_data["parcellations"][dataset][0]
                            derivation = volumetric_data["parcellations"][dataset][2]
                            atlasrelease_choice = volumetric_data["parcellations"][
                                dataset
                            ][3]
                            dataSampleModality = [volumetric_data["parcellations"][
                                dataset
                            ][4]]

                            # this is going ot be the "name" of the resource
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            break
                        else:
                            L.error(
                                f"Error: parcellation dataset '{filepath}' is not a "
                                "volumetric .nrrd file"
                            )
                            exit(1)
                except FileNotFoundError:
                    pass

        if not file_found:
            for dataset in volumetric_data["cell_orientations"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        if filepath.endswith(".nrrd"):
                            file_found = True
                            voxel_type = "vector"
                            resource_types = [resource_type, "nsg:CellOrientationField"]
                            description = (
                                f"{volumetric_data['cell_orientations'][dataset][1]}"
                            )
                            module_tag = volumetric_data["cell_orientations"][dataset][
                                0
                            ]
                            derivation = volumetric_data["cell_orientations"][dataset][
                                2
                            ]
                            atlasrelease_choice = volumetric_data["cell_orientations"][
                                dataset
                            ][3]
                            dataSampleModality = [volumetric_data["cell_orientations"][
                                dataset
                            ][4]]

                            # this is going ot be the "name" of the resource
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            break
                        else:
                            L.error(
                                f"Error: parcellation dataset '{filepath}' is not a "
                                "volumetric .nrrd file"
                            )
                            exit(1)
                except FileNotFoundError:
                    pass

        if not file_found:
            for dataset in volumetric_data["placement_hints"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        file_found = True
                        isPH = True
                        if os.path.isdir(filepath):
                            isFolder = True
                            directory = filepath
                            files = os.listdir(directory)
                            pattern = "*layer*"
                            files_list = fnmatch.filter(files, pattern)
                            files_list = sorted(files_list)
                            if len(files_list) != 6:
                                L.error(
                                    f"Error: '{filepath}' do not contain 6 placement "
                                    "hints volumetric files."
                                )
                                exit(1)
                            pattern = "*y.nrrd"
                            ylayer = fnmatch.filter(files, pattern)
                            if not ylayer:
                                L.error(
                                    f"Error: '{filepath}' do not contain the file "
                                    "[PH]y.nrrd volumetric files."
                                )
                                exit(1)
                            pattern = "*problematic_volume.nrrd"
                            mask_error = fnmatch.filter(files, pattern)
                            if not mask_error:
                                L.error(
                                    f"Error: '{filepath}' do not contain the "
                                    "problematic volume file."
                                )
                                exit(1)
                            pattern = "*report.json"
                            report_error = fnmatch.filter(files, pattern)
                            if not report_error:
                                L.error(
                                    f"Error: '{filepath}' do not contain the report "
                                    "json file"
                                )
                                exit(1)
                            files_list.extend(ylayer)
                            files_list.extend(mask_error)
                            files_list.extend(report_error)
                            # this is going ot be the "name" of the resource
                            filepath = os.path.join(directory, files_list[0])
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            voxel_type = "vector"
                            resource_types = [
                                resource_type,
                                "nsg:PlacementHintsDataLayer",
                            ]
                            layer_number = re.findall(
                                r"\d+", filename_noext.replace("_", " ").title()
                            )
                            description = volumetric_data["placement_hints"][dataset][1]
                            description = description.replace("XX", layer_number[0])
                            module_tag = volumetric_data["placement_hints"][dataset][0]
                            atlasrelease_choice = volumetric_data["placement_hints"][
                                dataset
                            ][3]
                            dataSampleModality = [volumetric_data["placement_hints"][
                                dataset
                            ][4]]

                            if (
                                dataset
                                == f"{volumes['placement_hints_hybrid_l23split']}"
                            ):
                                suffixe = "CCF v2-v3 Hybrid L23 Split"
                                annotation_description = description_hybrid_split
                            elif (
                                dataset
                                == f"{volumes['placement_hints_realigned_l23split']}"
                            ):
                                suffixe = "CCF v2-to-v3 Realigned L23 Split"
                                annotation_description = (
                                    description_realigned_split_simple
                                )
                            break
                        else:
                            L.error(
                                f"Error: placement hints dataset '{filepath}' is not a "
                                "folder containing placement hints volume and error "
                                "reports."
                            )
                            exit(1)

                except FileNotFoundError:
                    pass
        # If still no file found at this step then raise error
        if not file_found:
            L.error(
                f"Error: '{filepath}' does not correspond to one of the datasets "
                "defined in the VolumetricFile section of the 'generated dataset' "
                "configuration file"
            )
            exit(1)

        if provenances[0]:
            try:
                prov_description = append_provenance_to_description(
                    provenances, module_tag
                )
                description = f"{description} {prov_description}"
            except ValueError as e:
                L.error(f"ValueError in provenance content. {e}")
                exit(1)

        # Parsing the header of the NRRD file
        header = None
        try:
            header = nrrd.read_header(filepath)
        except nrrd.errors.NRRDError as e:
            L.error(f"NrrdError: {e}")
            L.info("Aborting pushing process.")  # setLevel(logging.INFO)
            exit(1)

        config = {
            "file_extension": file_extension,
            "sampling_space_unit": spatial_unit,
            "sampling_period": default_sampling_period,
            "sampling_time_unit": default_sampling_time_unit,
        }

        if not isinstance(forge._store, DemoStore):
            if not atlasrelease_dict["atlasrelease_choice"] or (
                atlasrelease_choice != atlasrelease_dict["atlasrelease_choice"]
            ):
                atlasrelease_dict["atlasrelease_choice"] = atlasrelease_choice
                atlasrelease_dict = return_atlasrelease(
                    forge,
                    config_content,
                    new_atlasrelease_hierarchy_path,
                    atlasrelease_dict,
                    parcellation_found,
                    atlas_reference_system_id,
                    subject,
                )

            if isinstance(atlasrelease_dict["atlas_release"], list):
                atlasRelease = [
                    {
                        "@id": atlasrelease_dict["atlas_release"][0].id,
                        "@type": ["AtlasRelease", "BrainAtlasRelease"],
                    },
                    {
                        "@id": atlasrelease_dict["atlas_release"][1].id,
                        "@type": ["AtlasRelease", "BrainAtlasRelease"],
                    },
                ]
                if atlasrelease_dict["create_new"]:
                    atlasrelease_dict["atlas_release"][0].contribution = contribution
                    atlasrelease_dict["atlas_release"][1].contribution = contribution
                    atlasreleases["atlas_releases"].append(
                        atlasrelease_dict["atlas_release"][0]
                    )
                    atlasreleases["atlas_releases"].append(
                        atlasrelease_dict["atlas_release"][1]
                    )
            else:
                atlasRelease = {
                    "@id": atlasrelease_dict["atlas_release"].id,
                    "@type": ["AtlasRelease", "BrainAtlasRelease"],
                }

        name = filename_noext.replace("_", " ").title()

        if isPH:
            name = f"{name} {suffixe}"
            # brainLocation["layer"] = f"{layer_number[0]}"

        # We create a 1st payload that will be recycled in case of multiple files to
        # push
        content_type = f"application/{file_extension}"
        distribution_file = forge.attach(filepath, content_type)

        resource_types.append("Dataset")
        nrrd_resource = Resource(
            type=resource_types,
            name=name,
            distribution=distribution_file,
            description=description,
            isRegisteredIn=isRegisteredIn,
            brainLocation=brainLocation,
            atlasRelease=atlasRelease,
            dataSampleModality = dataSampleModality,
            subject=subject,
            contribution=contribution,
        )

        nrrd_resource = add_nrrd_props(nrrd_resource, header, config, voxel_type)

        if derivation:
            nrrd_resource.derivation = derivation

        try:
            if os.path.samefile(
                volumes["cell_orientations_hybrid"], dataset
            ) or os.path.samefile(volumes["cell_orientations_realigned"], dataset):
                nrrd_resource.dimension[0]["name"] = "quaternion"
        except FileNotFoundError:
            pass

        # Link the atlasrelease to its parcellation
        if not isinstance(forge._store, DemoStore):
            if new_atlasrelease_hierarchy_path:
                try:
                    if os.path.samefile(volumes["annotation_hybrid_l23split"], dataset):
                        nrrd_resource.id = forge.format(
                            "identifier", "brainparcellationdatalayer", str(uuid4())
                        )
                        atlasrelease_dict["atlas_release"].parcellationVolume = {
                            "@id": nrrd_resource.id,
                            "@type": [
                                "Dataset",
                                "VolumetricDataLayer",
                                "BrainParcellationDataLayer",
                            ],
                        }
                        atlasrelease_dict["atlas_release"].contribution = contribution
                        atlasreleases["atlas_releases"].append(
                            atlasrelease_dict["atlas_release"]
                        )
                        atlasrelease_dict["hierarchy"].contribution = contribution
                        atlasreleases["hierarchy"].append(
                            atlasrelease_dict["hierarchy"]
                        )
                except FileNotFoundError:
                    pass

                try:
                    if os.path.samefile(
                        volumes["annotation_realigned_l23split"], dataset
                    ):
                        nrrd_resource.id = forge.format(
                            "identifier", "brainparcellationdatalayer", str(uuid4())
                        )
                        atlasrelease_dict["atlas_release"].parcellationVolume = {
                            "@id": nrrd_resource.id,
                            "@type": "BrainParcellationDataLayer",
                        }
                        atlasrelease_dict["atlas_release"].contribution = contribution
                        atlasreleases["atlas_releases"].append(
                            atlasrelease_dict["atlas_release"]
                        )
                except FileNotFoundError:
                    pass

        datasets.append(nrrd_resource)

        # If the input is a folder containing the cell density file to push
        if isFolder:
            for f in range(1, len(files_list)):  # start at the 2nd file
                filepath = os.path.join(directory, files_list[f])
                filename_noext = os.path.splitext(os.path.basename(filepath))[0]
                name = filename_noext.replace("_", " ").title()
                distribution_file = forge.attach(filepath, content_type)

                if isCellDensity:
                    cell_density_file = filename_noext.replace("_", " ")
                    cell_density_name = (
                        f"{cell_density_file[0].upper()}" f"{cell_density_file[1:]}"
                    )
                    description = (
                        f"{cell_density_name} volume for the "
                        f"{volumetric_data['cell_densities'][dataset][1]}."
                    )

                if isPH:
                    name = f"{name} {suffixe}"
                    if f <= 5:
                        layer_number = re.findall(r"\d+", files_list[f])
                        description = volumetric_data["placement_hints"][dataset][1]
                        description = description.replace("XX", layer_number[0])
                    # else:
                    #     try:
                    #         brainLocation.pop("layer")
                    #     except KeyError:
                    #         pass
                    if f == 6:
                        description = "Volume containing for each voxel its distance "
                        f"from the bottom of the {annotation_description} Isocortex. "
                        "The bottom being the deepest part of the Isocortex (highest "
                        "cortical depth)."
                    if f == 7:
                        description = (
                            f"3D mask volume of the {annotation_description}. It "
                            "includes Isocortex problematic voxels along with the "
                            "distance JSON report containing the proportion of "
                            "problematic voxels. Problematic voxels are highlighted "
                            "during the placement-hints computation and correspond to "
                            "voxels with at least one distance-related problem (i.e "
                            "those who do not intersect with the bottom or top mesh, "
                            "those with a distance gap greater than the maximum "
                            "thickness...)."
                        )
                        distribution_mask = forge.attach(filepath, content_type)
                        report_json = os.path.join(directory, files_list[8])
                        content_type = (
                            f"application/"
                            f"{os.path.splitext(os.path.basename(report_json))[1][1:]}"
                        )
                        distribution_json = forge.attach(report_json, content_type)
                        distribution_file = [distribution_mask, distribution_json]

                if provenances[0]:
                    description = f"{description} {prov_description}"
                # Use forge.reshape instead ?
                nrrd_resources = Resource(
                    type=nrrd_resource.type,
                    name=name,
                    distribution=distribution_file,
                    description=description,
                    contribution=nrrd_resource.contribution,
                    isRegisteredIn=nrrd_resource.isRegisteredIn,
                    brainLocation=nrrd_resource.brainLocation,
                    atlasRelease=nrrd_resource.atlasRelease,
                    componentEncoding=nrrd_resource.componentEncoding,
                    fileExtension=nrrd_resource.fileExtension,
                    dimension=nrrd_resource.dimension,
                    sampleType=nrrd_resource.sampleType,
                    worldMatrix=nrrd_resource.worldMatrix,
                    resolution=nrrd_resource.resolution,
                    bufferEncoding=nrrd_resource.bufferEncoding,
                    endianness=nrrd_resource.endianness,
                    dataSampleModality = nrrd_resource.dataSampleModality,
                    subject=nrrd_resource.subject,
                )

                if isPH:
                    if 5 < f < 8:
                        try:
                            header = nrrd.read_header(filepath)
                        except nrrd.errors.NRRDError as e:
                            L.error(f"NrrdError: {e}")
                            L.info("Aborting pushing process.")
                            exit(1)
                        config["file_extension"] = os.path.splitext(
                            os.path.basename(files_list[f])
                        )[1][1:]
                        voxel_type = "label"
                        nrrd_resources = add_nrrd_props(
                            nrrd_resources, header, config, voxel_type
                        )

                datasets.append(nrrd_resources)
                if isPH:
                    if f >= 7:
                        nrrd_resources.dataSampleModality=["mask"]
                        break

    return datasets, atlasreleases


def return_atlasrelease(
    forge,
    config_content,
    new_atlasrelease_hierarchy_path,
    atlasrelease_dict,
    parcellation_found,
    atlas_reference_system_id,
    subject,
):

    spatialReferenceSystem = {
        "@id": "https://bbp.epfl.ch/neurosciencegraph/data/"
        "allen_ccfv3_spatial_reference_system",
        "@type": "AtlasSpatialReferenceSystem",
    }

    # average brain model ccfv3
    brainTemplateDataLayer = {
        "@id": "https://bbp.epfl.ch/neurosciencegraph/data/"
        "dca40f99-b494-4d2c-9a2f-c407180138b7",
        "@type": "BrainTemplateDataLayer",
    }

    releaseDate = {
        "@type": "xsd:date",
        "@value": f"{datetime.today().strftime('%Y-%m-%d')}",
    }
    link_to_hierarchy = False
    atlasrelease_resource = []
    if atlasrelease_dict["atlasrelease_choice"] == "atlasrelease_hybrid":
        if not new_atlasrelease_hierarchy_path:
            # Atlas release hybrid v2-v3 L2L3 split
            try:
                filters = {"name": "Allen Mouse CCF v2-v3 hybrid l2-l3 split"}
                atlasrelease_resource = forge.search(filters, limit=1)[0]
                atlasrelease_dict["atlas_release"] = atlasrelease_resource
            except Exception as e:
                L.error(
                    "Error when searching the BrainAtlasRelease Resource 'Allen "
                    "Mouse CCF v2-v3 hybrid l2-l3 split' in the destination "
                    f"project '{forge._store.bucket}'. {e}"
                )
                exit(1)
        elif "annotation_hybrid_l23split" in parcellation_found:
            description = (
                "This atlas release uses the brain parcellation resulting of the "
                "hybridation between CCFv2 and CCFv3 and integrating the splitting of "
                "layer 2 and layer 3. The average brain template and the ontology is "
                "common across CCFv2 and CCFv3."
            )
            atlasrelease_resource = Resource(
                id=forge.format("identifier", "brainatlasrelease", str(uuid4())),
                type=["AtlasRelease", "BrainAtlasRelease"],
                name="Allen Mouse CCF v2-v3 hybrid l2-l3 split",
                description=description,
                brainTemplateDataLayer=brainTemplateDataLayer,
                spatialReferenceSystem=spatialReferenceSystem,
                subject=subject,
                releaseDate=releaseDate,
            )
            link_to_hierarchy = True
        if not atlasrelease_resource:
            L.error(
                "No BrainAtlasRelease 'Allen Mouse CCF v2-v3 hybrid l2-l3 "
                "split' resource found in the destination project "
                f"'{forge._store.bucket}'. Please provide the argument "
                "--new-atlasrelease-hierarchy-path and the right parcellation volume "
                "to first generate and push a new atlas release resource into your "
                "project ."
            )
            exit(1)

    # Atlas Releases realigned split volume
    elif atlasrelease_dict["atlasrelease_choice"] == "atlasrelease_realigned":
        if not new_atlasrelease_hierarchy_path:
            try:
                filters = {"name": "Allen Mouse CCF v2-v3 realigned l2-l3 split"}
                atlasrelease_resource = forge.search(filters, limit=1)[0]
                atlasrelease_dict["atlas_release"] = atlasrelease_resource
            except Exception as e:
                L.error(
                    "Error when searching the BrainAtlasRelease Resource 'Allen "
                    "Mouse CCF v2-v3 realigned l2-l3 split' in the destination "
                    f"project '{forge._store.bucket}'. {e}"
                )
                exit(1)
        elif "annotation_realigned_l23split" in parcellation_found:
            description = (
                "This atlas release uses the brain parcellation resulting of the "
                "realignment of CCFv2 over CCFv3 and integrating the splitting of "
                "layer 2 and layer 3. The average brain template and the ontology is "
                "common across CCFv2 and CCFv3."
            )
            atlasrelease_resource = Resource(
                id=forge.format("identifier", "brainatlasrelease", str(uuid4())),
                type=["AtlasRelease", "BrainAtlasRelease"],
                name="Allen Mouse CCF v2-v3 realigned l2-l3 split",
                description=description,
                brainTemplateDataLayer=brainTemplateDataLayer,
                spatialReferenceSystem=spatialReferenceSystem,
                subject=subject,
                releaseDate=releaseDate,
            )
            link_to_hierarchy = True
        if not atlasrelease_resource:
            L.error(
                "No BrainAtlasRelease 'Allen Mouse CCF v2-v3 realigned l2-l3 "
                "split' resource found in the destination project "
                f"'{forge._store.bucket}'. Please provide the argument "
                "--new-atlasrelease-hierarchy-path and the right parcellation volume "
                "to first generate and push a new atlas release resource into your "
                "project."
            )
            exit(1)

    # Old Atlas Releases ccfv2 and ccfv3
    elif atlasrelease_dict["atlasrelease_choice"] == "atlasrelease_ccfv2v3":
        try:
            filters = {"name": "Allen Mouse CCF v2"}
            atlasreleasev2_resource = forge.search(filters, limit=1)[0]
            filters = {"name": "Allen Mouse CCF v3"}
            atlasreleasev3_resource = forge.search(filters, limit=1)[0]
            atlasrelease_dict["atlas_release"] = [
                atlasreleasev2_resource,
                atlasreleasev3_resource,
            ]
            atlasrelease_dict["create_new"] = False
        except Exception as e:
            L.error(
                "Error when searching the BrainAtlasRelease Resources 'Allen "
                "Mouse CCF v2' and 'Allen Mouse CCF v3'in the destination "
                f"project '{forge._store.bucket}'. {e}"
            )
            exit(1)
        if not atlasreleasev2_resource or not atlasreleasev3_resource:
            L.info(
                "No BrainAtlasRelease 'Allen Mouse CCF v2' and 'Allen "
                "Mouse CCF v3' resources found in the destination project "
                f"'{forge._store.bucket}'. They will therefore be created."
            )
            description_ccfv2 = (
                "This atlas release uses the brain parcellation of CCFv2 (2011). The "
                "average brain template and the ontology is common across CCFv2 and "
                "CCFv3."
            )
            name_ccfv2 = "Allen Mouse CCF v2"
            parcellationOntology = {
                "@id": "http://bbp.epfl.ch/neurosciencegraph/ontologies/mba",
                "@type": ["Entity", "Ontology", "ParcellationOntology"],
            }
            parcellationVolume = {
                "@id": "https://bbp.epfl.ch/neurosciencegraph/data/ "
                "7b4b36ad-911c-4758-8686-2bf7943e10fb",
                "@type": [
                    "Dataset",
                    "VolumetricDataLayer",
                    "BrainParcellationDataLayer",
                ],
            }

            atlasreleasev2_resource = Resource(
                id=forge.format("identifier", "brainatlasrelease", str(uuid4())),
                type=["AtlasRelease", "BrainAtlasRelease"],
                name=name_ccfv2,
                description=description_ccfv2,
                brainTemplateDataLayer=brainTemplateDataLayer,
                spatialReferenceSystem=spatialReferenceSystem,
                subject=subject,
                parcellationOntology=parcellationOntology,
                parcellationVolume=parcellationVolume,
                releaseDate=releaseDate,
            )

            atlasreleasev3_resource = Resource(
                id=forge.format("identifier", "brainatlasrelease", str(uuid4())),
                type=["AtlasRelease", "BrainAtlasRelease"],
                name=name_ccfv2.replace("v2", "v3"),
                description=description_ccfv2.replace("CCFv2 (2011)", "CCFv3 (2017)"),
                brainTemplateDataLayer=brainTemplateDataLayer,
                spatialReferenceSystem=spatialReferenceSystem,
                subject=subject,
                parcellationOntology=parcellationOntology,
                parcellationVolume=parcellationVolume,
                releaseDate=releaseDate,
            )
            atlasrelease_dict["atlas_release"] = [
                atlasreleasev2_resource,
                atlasreleasev3_resource,
            ]
            atlasrelease_dict["create_new"] = True

    # Link the new atlas release to the hierarchy file
    if new_atlasrelease_hierarchy_path and link_to_hierarchy:
        if not atlasrelease_dict["hierarchy"]:
            try:
                if os.path.samefile(
                    new_atlasrelease_hierarchy_path,
                    config_content["HierarchyJson"]["hierarchy_l23split"],
                ):
                    pass
                else:
                    L.error(
                        "Error: The atlas regions hierarchy file provided does not "
                        "correspond to 'hierarchy_l23split' from the dataset "
                        "configuration file"
                    )
                    exit(1)
            except FileNotFoundError as error:
                L.error(f"Error: {error}")
                exit(1)

            description = (
                "AIBS Mouse CCF Atlas regions hierarchy tree file including the split "
                "of layer 2 and layer 3"
            )
            # Original AIBS hierarchy file
            # "@type": ["Entity", "Ontology"],
            derivation = {
                "@type": "Derivation",
                "entity": {
                    "@id": "http://bbp.epfl.ch/neurosciencegraph/ontologies/mba",
                    "@type": "Entity",
                },
            }
            file_extension = os.path.splitext(
                os.path.basename(new_atlasrelease_hierarchy_path)
            )[1][1:]

            content_type = f"application/{file_extension}"
            distribution_file = forge.attach(
                new_atlasrelease_hierarchy_path, content_type
            )

            hierarchy_resource = Resource(
                id=forge.format("identifier", "parcellationontology", str(uuid4())),
                type=["Entity", "Ontology", "ParcellationOntology"],
                name="AIBS Mouse CCF Atlas parcellation ontology L2L3 split",
                distribution=distribution_file,
                description=description,
                derivation=derivation,
                subject=subject,
            )

            hierarchy_resource.label = hierarchy_resource.name
            atlasrelease_resource.parcellationOntology = {
                "@id": hierarchy_resource.id,
                "@type": ["Entity", "ParcellationOntology", "Ontology"],
            }
            atlasrelease_dict["atlas_release"] = atlasrelease_resource
            atlasrelease_dict["hierarchy"] = hierarchy_resource
        else:
            atlasrelease_resource.parcellationOntology = {
                "@id": atlasrelease_dict["hierarchy"].id,
                "@type": ["Entity", "ParcellationOntology", "Ontology"],
            }
            atlasrelease_dict["atlas_release"] = atlasrelease_resource

    return atlasrelease_dict


def add_nrrd_props(resource, nrrd_header, config, voxel_type):
    """
    Add to the resource all the fields expected for a VolumetricDataLayer/NdRaster
    that can be found in the NRRD header. A resource dictionary must exist and be
    provided (even if empty).

    Parameters:
        resource : Resource object defined by a properties payload linked to a file.
        nrrd_header : Dict containing the input file header fields  and their
        corresponding value.
        config : Dict containing the file extension and its sampling informations.
        voxel_type : String indicating the type of voxel contained in the volumetric
                     dataset.

    Returns:
        dataset : Resource object with all Nrrd properties added.
    """

    NRRD_TYPES_TO_NUMPY = {
        "signed char": "int8",
        "int8": "int8",
        "int8_t": "int8",
        "uchar": "uint8",
        "unsigned char": "uint8",
        "uint8": "uint8",
        "uint8_t": "uint8",
        "short": "int16",
        "short int": "int16",
        "signed short": "int16",
        "signed short int": "int16",
        "int16": "int16",
        "int16_t": "int16",
        "ushort": "int16",
        "unsigned short": "uint16",
        "unsigned short int": "uint16",
        "uint16": "uint16",
        "uint16_t": "uint16",
        "int": "int32",
        "signed int": "int32",
        "int32": "int32",
        "int32_t": "int32",
        "uint": "uint32",
        "unsigned int": "uint32",
        "uint32": "uint32",
        "uint32_t": "uint32",
        "longlong": "int64",
        "long long": "int64",
        "long long int": "int64",
        "signed long long": "int64",
        "signed long long int": "int64",
        "int64": "int64",
        "int64_t": "int64",
        "ulonglong": "uint64",
        "unsigned long long": "uint64",
        "unsigned long long int": "uint64",
        "uint64": "uint64",
        "uint64_t": "uint64",
        "float": "float32",
        "double": "float64",
    }

    space_origin = None
    if "space origin" in nrrd_header:
        space_origin = nrrd_header["space origin"].tolist()
    else:
        if nrrd_header["dimension"] == 2:
            space_origin = [0.0, 0.0]
        elif nrrd_header["dimension"] == 3:
            space_origin = [0.0, 0.0, 0.0]

    space_directions = None
    if "space directions" in nrrd_header:
        # replace the nan that pynrrd adds to None (just like in NRRD spec)
        space_directions = []
        for col in nrrd_header["space directions"].tolist():
            if np.isnan(col).any():
                space_directions.append(None)
            else:
                space_directions.append(col)

    # Here, 'space directions' being missing in the file, we hardcode an identity matrix
    # If we have 4 dimensions, we say
    else:
        if nrrd_header["dimension"] == 2:
            space_directions = [[1, 0], [0, 1]]
        elif nrrd_header["dimension"] == 3:
            space_directions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
        elif nrrd_header["dimension"] == 4:
            # the following is a very lousy way to determine if among the 4 dims,
            # or the first is components or the last is time...
            if nrrd_header["sizes"][0] < (np.mean(nrrd_header["sizes"] * 0.20)):
                space_directions = [None, [1, 0, 0], [0, 1, 0], [0, 0, 1]]  # component
            else:
                space_directions = [[1, 0, 0], [0, 1, 0], [0, 0, 1], None]  # time

        elif nrrd_header["dimension"] == 5:
            space_directions = [None, [1, 0, 0], [0, 1, 0], [0, 0, 1], None]

    resource.componentEncoding = NRRD_TYPES_TO_NUMPY[nrrd_header["type"]]
    resource.endianness = nrrd_header["endian"]
    resource.bufferEncoding = nrrd_header["encoding"]
    resource.fileExtension = config["file_extension"]
    resource.dimension = []

    component_dim_index = -1
    passed_spatial_dim = False
    # for each dimension
    for i in range(0, nrrd_header["dimension"]):
        current_dim = {}
        current_dim["size"] = nrrd_header["sizes"][i].item()

        # this is a spatial dim
        if space_directions[i]:
            passed_spatial_dim = True
            current_dim["@type"] = "SpaceDimension"
            current_dim["unitCode"] = config["sampling_space_unit"]

        # this can be a component or a time dim
        else:
            # this is a time dim as it is located after space dim)
            if passed_spatial_dim:
                current_dim["@type"] = "nsg:TimeDimension"
                current_dim["samplingPeriod"] = config["sampling_period"]
                current_dim["unitCode"] = config["sampling_time_unit"]

            # this is a component dim as it is located before space dim
            else:
                # decide of the label
                component_dim_index = i
                current_dim["@type"] = "ComponentDimension"
                # current_dim["name"] = default_sample_type_multiple_components if
                # current_dim["size"] > 1 else default_sample_type_single_component
                try:
                    current_dim["name"] = get_voxel_type(
                        voxel_type, current_dim["size"]
                    )
                except ValueError as e:
                    L.error(f"ValueError: {e}")
                    exit(1)
                except KeyError as e:
                    L.error(f"KeyError: {e}")
                    exit(1)

        resource.dimension.append(current_dim)

    # repeating the name of the component dimension in the "sampleType" base level prop
    if component_dim_index >= 0:
        resource.sampleType = resource.dimension[component_dim_index]["name"]

    # As no component dim was mentioned in metadata, it means the component is of size 1
    else:
        # prepend a dimension component
        try:
            name = get_voxel_type(voxel_type, 1)
        except ValueError as e:
            L.error(f"ValueError: {e}")
            exit(1)
        component_dim = {"@type": "ComponentDimension", "size": 1, "name": name}
        resource.dimension.insert(0, component_dim)

        resource.sampleType = component_dim["name"]

    # creating the world matrix (column major)
    # 1. pynrrd creates a [nan, nan, nan] line for each 'space directions' that is
    # 'none' in the header.
    # We have to strip them off.
    worldMatrix = None
    r = []  # rotation mat
    o = space_origin
    for col in space_directions:
        if col is not None:
            r.append(col)

    # if 3D, we create a 4x4 homogeneous transformation matrix
    if len(r) == 3:
        worldMatrix = [
            r[0][0],
            r[0][1],
            r[0][2],
            0,
            r[1][0],
            r[1][1],
            r[1][2],
            0,
            r[2][0],
            r[2][1],
            r[2][2],
            0,
            o[0],
            o[1],
            o[2],
            1,
        ]

    # if 2D, we create a 3x3 homogeneous transformation matrix
    if len(r) == 2:
        worldMatrix = [r[0][0], r[0][1], 0, r[1][0], r[1][1], 0, o[0], o[1], 1]

    # nesting the matrix values into object with @value props
    for i in range(0, len(worldMatrix)):
        # worldMatrix[i] = {"@value": float(worldMatrix[i])}
        worldMatrix[i] = float(worldMatrix[i])

    resource.worldMatrix = worldMatrix

    resource.resolution = {"value": r[0][0], "unitCode": config["sampling_space_unit"]}

    return resource
