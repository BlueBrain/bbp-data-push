"""
Create a 'VolumetricDataLayer', an 'atlasRelease' and an 'ontology' resource payload
to push into Nexus. If the resources already exist in Nexus, they will be fetched and
updated instead.
This script has been designed to function with volumetric files generated by the Atlas
pipeline.
To know more about 'VolumetricDataLayer' resources and Nexus, see
https://bluebrainnexus.io.
Link to BBP Atlas pipeline confluence documentation:
https://bbpteam.epfl.ch/project/spaces/x/rS22Ag
"""

import os
import yaml
import json
import numpy as np
import nrrd
import fnmatch
import re
from uuid import uuid4
import copy
from kgforge.core import Resource
from kgforge.specializations.stores.demo_store import DemoStore

# from kgforge.core.commons.exceptions import RetrievalError

from bba_data_push.commons import (
    get_voxel_type,
    get_brain_region_prop,
    get_hierarchy_file,
    return_contribution,
    return_atlasrelease,
    return_activity_payload,
    return_file_hash,
    fetch_linked_resources,
)
import bba_data_push.constants as const
from bba_data_push.logging import create_log_handler

L = create_log_handler(__name__, "./push_nrrd_volumetricdatalayer.log")


def create_volumetric_resources(
    forge,
    inputpath: list,
    config_path,
    atlasrelease_config_path,
    input_hierarchy,
    input_hierarchy_jsonld,
    provenance_metadata_path,
    link_regions_path,
    resource_tag,
    verbose,
) -> list:
    """
    Construct the input volumetric dataset, atlasrelease and hierarchy payloads that
    will be push with the corresponding files into Nexus as a resource.

    Parameters:
        forge : instantiated and configured forge object.
        inputpath : input datasets paths. These datasets are either volumetric files
                    or folder containing volumetric files.
        config_path : configuration yaml file path containing the names and paths of
                    the Atlas Annotation pipeline generated datasets.
        atlasrelease_config_path : Json file containing the atlasRelease @id as well
                    as its ontology and parcellation volume @id. It needs to contains
                    at least these informations for the atlasRelease Allen Mouse CCFV2
                    and CCFV3 stocked in the Nexus project bbp/atlas.
        input_hierarchy : hierarchy json files.
        input_hierarchy_jsonld : hierarchy jsonld file to be attached to the
                    atlasrelease ontology.
        provenance_metadata_path : configuration json file containing various
                    information about dataset provenance generated from the Atlas
                    Annotation Pipeline run.
        link_regions_path : Json file meant to contain the @ids of the brain regions
                    masks, meshes and region summaries.
        resource_tag : Tag value (string).
        verbose : Verbosity level.
    Returns:
        resources_payloads : dict of the form containing the Resource objects
                (volumetricdatalayer, atlasrelease, hierarchy, activity) that has been
                constructed and need to be updated/pushed in Nexus.
    """
    L.setLevel(verbose)

    config_file = open(config_path)
    config_content = yaml.safe_load(config_file.read().strip())
    config_file.close()
    try:
        volumes = config_content["GeneratedDatasetPath"]["VolumetricFile"]
        hierarchies = config_content["HierarchyJson"]
    except KeyError as error:
        L.error(f"KeyError: {error} is not found in the dataset configuration file.")
        exit(1)

    if provenance_metadata_path:
        try:
            with open(provenance_metadata_path, "r") as f:
                provenance_metadata = json.loads(f.read())
        except ValueError as error:
            L.error(f"{error} : {provenance_metadata_path}.")
            exit(1)

        deriv_dict_id = create_deriv_dict_id(
            forge, inputpath, provenance_metadata, volumes, hierarchies
        )
    else:
        provenance_metadata = None
        deriv_dict_id = {}

    # Dict containing all the pipeline generated volumetric datasets and their
    # informations
    try:
        volumetric_dict = const.return_volumetric_dict(volumes)
    except KeyError as error:
        L.error(f"{error}")
        exit(1)

    # Create contribution
    if isinstance(forge._store, DemoStore):
        contribution = []
    else:
        try:
            contribution, log_info = return_contribution(forge)
            L.info("\n".join(log_info))
        except Exception as e:
            L.error(f"{e}")
            exit(1)

    deriv_celldensities_template = {
        "@type": "Derivation",
        "entity": {
            "@id": "",
            "@type": "Dataset",
        },
    }

    # Constants

    resources_payloads = {
        "datasets_toUpdate": {
            f"{const.schema_volumetricdatalayer}": [],
            f"{const.schema_atlasrelease}": [],
            f"{const.schema_ontology}": [],
            f"{const.schema_spatialref}": [],
        },
        "datasets_toPush": {
            f"{const.schema_volumetricdatalayer}": [],
            f"{const.schema_atlasrelease}": [],
            f"{const.schema_ontology}": [],
            f"{const.schema_spatialref}": [],
        },
        "activity": [],
        "tag": "",
    }

    # Return the atlas spatial reference system resource
    spatialref_resource = const.return_spatial_reference(forge)
    spatialref_resource.contribution = contribution
    if spatialref_resource._store_metadata:
        resources_payloads["datasets_toUpdate"][const.schema_spatialref].append(
            spatialref_resource
        )
    else:
        resources_payloads["datasets_toPush"][const.schema_spatialref].append(
            spatialref_resource
        )

    atlasrelease_payloads = {
        "atlasrelease_choice": None,
        "atlas_release": None,
        "hierarchy": None,
        "tag": None,
        "fetched": False,
        "aibs_atlasrelease": False,
    }
    atlasrelease_choosen = []
    atlasRelease = {}
    dict_ids = {}
    generation = {}
    activity_resource = []
    atlasrelease_parcellation = None
    for filepath in inputpath:
        fileFound = False
        isFolder = False
        resource_flag = ""
        derivation = False
        derivationDataFound = False
        dimension_name = False
        flat_tree = {}
        action_summary_file = False
        link_summary_content = {}
        differentAtlasrelease = False
        dataset_name = None
        distribution_file = None
        fetched_resources = None
        toUpdate = False
        fetched_resource_id = None
        fetched_resource_metadata = None
        parcellationAtlas_id = None
        region_id = 997  # default: 997 --> root, whole brain
        region_name = "root"
        brainLocation = {
            "brainRegion": {"@id": f"mba:{region_id}", "label": f"{region_name}"},
            "atlasSpatialReferenceSystem": {
                "@type": [
                    "BrainAtlasSpatialReferenceSystem",
                    "AtlasSpatialReferenceSystem",
                ],
                "@id": const.atlas_spatial_reference_system_id,
            },
        }
        # ============================== CELL DENSITIES ==============================
        # OUTDATED. Oldschool messy derivation links. The goal was to add to the
        # volumetric cell density file payload the derivation to the cell density
        # dataset that is the folder containing the cell density volumetric files
        for dataset in volumetric_dict["cell_densities"]:
            try:
                if os.path.samefile(filepath, dataset):
                    fileFound = True
                    resource_flag = "isCellDensity"
                    dataset_dict = volumetric_dict["cell_densities"][dataset]
                    if os.path.isdir(filepath):
                        isFolder = True
                        directory = filepath
                        files = os.listdir(directory)
                        pattern = "*_density.nrrd"
                        files_list = fnmatch.filter(files, pattern)
                        if not files_list:
                            L.error(
                                f"Error: '{filepath}' do not contain any cell density "
                                "volumetric files."
                            )
                            exit(1)
                        # this is going ot be the "name" of the resource
                        filepath = os.path.join(directory, files_list[0])
                        filename_noext = os.path.splitext(os.path.basename(filepath))[0]
                        file_extension = os.path.splitext(os.path.basename(filepath))[
                            1
                        ][1:]
                        file_split = filename_noext.split("_")
                        v = "mtypes_densities_probability_map_ccfv2_correctednissl"
                        try:
                            if os.path.samefile(
                                directory,
                                volumes[
                                    "mtypes_densities_profile_ccfv2_correctednissl"
                                ],
                            ) or os.path.samefile(
                                directory,
                                volumes[v],
                            ):
                                file_split.insert(0, "Mtype")
                        except FileNotFoundError:
                            pass
                        cell_density_file = " ".join(file_split)
                        atlasrelease_choice = dataset_dict["atlasrelease"]
                        filename_noext = cell_density_file
                        if atlasrelease_choice == "atlasrelease_ccfv2":
                            filename_noext = f"{filename_noext} Ccfv2 Corrected Nissl"
                        description = (
                            f"{cell_density_file[0].upper()}{cell_density_file[1:]} "
                            "volume for the "
                            f"{dataset_dict['description']}."
                        )
                        voxel_type = dataset_dict["voxel_type"]
                        resource_types = dataset_dict["type"]

                        if dataset_dict["derivation"]:
                            if not isinstance(
                                dataset_dict["derivation"],
                                dict,
                            ):
                                if isinstance(
                                    dataset_dict["derivation"],
                                    tuple,
                                ):
                                    data_deriv = dataset_dict["derivation"][0]
                                    subdata_deriv = dataset_dict["derivation"][1]
                                    fulldata_deriv = f"{data_deriv}/{subdata_deriv}"
                                else:
                                    data_deriv = dataset_dict["derivation"]
                                # Search for the file to derive from in the input files
                                # and if it is already a resource or not
                                for inputdata in inputpath:
                                    if os.path.samefile(
                                        data_deriv,
                                        inputdata,
                                    ):
                                        derivationDataFound = True
                                        for r in range(
                                            0, len(resources_payloads["datasets"])
                                        ):
                                            distrib = resources_payloads["datasets"][
                                                r
                                            ].distribution.args[0]
                                            if isinstance(
                                                dataset_dict["derivation"],
                                                tuple,
                                            ):
                                                data_deriv = fulldata_deriv
                                            # check if a previous resource is the one
                                            # from which it derives
                                            if os.path.samefile(data_deriv, distrib):
                                                # Check if the previous resource has
                                                # an id already
                                                try:
                                                    if resources_payloads["datasets"][
                                                        r
                                                    ].id:
                                                        pass
                                                except AttributeError:
                                                    dataset_id = forge.format(
                                                        "identifier",
                                                        "volumetricdatalayer",
                                                        str(uuid4()),
                                                    )
                                                    resources_payloads["datasets"][
                                                        r
                                                    ].id = f"{dataset_id}"
                                                derivation = copy.deepcopy(
                                                    deriv_celldensities_template
                                                )
                                                derivation["entity"][
                                                    "@id"
                                                ] = f"{dataset_id}"
                                                break
                                        # if the parent resource has not been created
                                        # yet
                                        if not derivation:
                                            dict_ids[f"{data_deriv}"] = forge.format(
                                                "identifier",
                                                "volumetricdatalayer",
                                                str(uuid4()),
                                            )
                                            derivation = copy.deepcopy(
                                                deriv_celldensities_template
                                            )
                                            derivation["entity"]["@id"] = dict_ids[
                                                f"{data_deriv}"
                                            ]
                                if not derivationDataFound:
                                    L.info(
                                        f"The file '{data_deriv}' whose "
                                        "resource corresponding to the file "
                                        f"'{dataset}' derivates is absent from the "
                                        "input dataset. The property 'derivation' "
                                        "from the latter resource will be thus left "
                                        "empty."
                                    )
                            else:
                                derivation = dataset_dict["derivation"]
                        dataset_name = dataset_dict["name"]
                        dataSampleModality = dataset_dict["datasamplemodality"]
                        break
                    else:
                        if filepath.endswith(".nrrd"):
                            fileFound = True
                            # this is going ot be the "name" of the resource
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            file_split = filename_noext.split("_")[:3]
                            cell_density_file = " ".join(file_split)
                            atlasrelease_choice = dataset_dict["atlasrelease"]
                            filename_noext = cell_density_file
                            if atlasrelease_choice == "atlasrelease_ccfv2":
                                filename_noext = (
                                    f"{filename_noext} Ccfv2 Corrected Nissl"
                                )
                            description = (
                                f"{cell_density_file[0].upper()}"
                                f"{cell_density_file[1:]} volume for the "
                                f"{dataset_dict['description']}."
                            )
                            voxel_type = dataset_dict["voxel_type"]
                            resource_types = dataset_dict["type"]
                            # Derivation
                            if dataset_dict["derivation"]:
                                if not isinstance(
                                    dataset_dict["derivation"],
                                    dict,
                                ):

                                    # Search for the file to derive from in the input
                                    # files and if it is already a resource or not
                                    if isinstance(
                                        dataset_dict["derivation"],
                                        tuple,
                                    ):
                                        data_deriv = dataset_dict["derivation"][0]
                                        subdata_deriv = dataset_dict["derivation"][1]
                                        fulldata_deriv = f"{data_deriv}/{subdata_deriv}"
                                    else:
                                        data_deriv = dataset_dict["derivation"]
                                    for inputdata in inputpath:
                                        if os.path.samefile(
                                            data_deriv,
                                            inputdata,
                                        ):
                                            derivationDataFound = True
                                            for r in range(
                                                0, len(resources_payloads["datasets"])
                                            ):
                                                distrib = resources_payloads[
                                                    "datasets"
                                                ][r].distribution.args[0]
                                                if isinstance(
                                                    dataset_dict["derivation"],
                                                    tuple,
                                                ):
                                                    data_deriv = fulldata_deriv
                                                if os.path.samefile(
                                                    data_deriv, distrib
                                                ):
                                                    try:
                                                        if resources_payloads[
                                                            "datasets"
                                                        ][r].id:
                                                            pass
                                                    except AttributeError:
                                                        resources_payloads["datasets"][
                                                            r
                                                        ].id = forge.format(
                                                            "identifier",
                                                            "volumetricdatalayer",
                                                            str(uuid4()),
                                                        )
                                                    derivation = copy.deepcopy(
                                                        deriv_celldensities_template
                                                    )
                                                    derivation["entity"][
                                                        "@id"
                                                    ] = resources_payloads["datasets"][
                                                        r
                                                    ].id
                                                    break
                                            if not derivation:
                                                dict_ids[
                                                    f"{data_deriv}"
                                                ] = forge.format(
                                                    "identifier",
                                                    "volumetricdatalayer",
                                                    str(uuid4()),
                                                )
                                                derivation = copy.deepcopy(
                                                    deriv_celldensities_template
                                                )
                                                derivation["entity"]["@id"] = dict_ids[
                                                    f"{data_deriv}"
                                                ]
                                    if not derivationDataFound:
                                        L.info(
                                            f"The file '{data_deriv}' whose "
                                            "resource corresponding to the file "
                                            f"'{dataset}' derivates is absent from the "
                                            "input dataset. The property 'derivation' "
                                            "from the latter resource will be thus "
                                            "left empty."
                                        )
                                else:
                                    derivation = dataset_dict["derivation"]
                            dataset_name = dataset_dict["name"]
                            dataSampleModality = dataset_dict["datasamplemodality"]
                            break
                        else:
                            L.error(
                                f"Error: parcellation dataset '{filepath}' is not a "
                                "volumetric .nrrd file"
                            )
                            exit(1)
            except FileNotFoundError:
                pass

        # ============================== PARCELLATIONS ==============================
        if not fileFound:
            for dataset in volumetric_dict["parcellations"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        if filepath.endswith(".nrrd"):
                            fileFound = True
                            dataset_dict = volumetric_dict["parcellations"][dataset]
                            resource_types = dataset_dict["type"]
                            voxel_type = dataset_dict["voxel_type"]
                            description = f"{dataset_dict['description']}"
                            atlasrelease_choice = dataset_dict["atlasrelease"]
                            dataSampleModality = dataset_dict["datasamplemodality"]
                            dataset_name = dataset_dict["name"]
                            # this is going ot be the "name" of the resource
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            break
                        else:
                            L.error(
                                f"Error: parcellation dataset '{filepath}' is not a "
                                "volumetric .nrrd file"
                            )
                            exit(1)
                except FileNotFoundError:
                    pass
        # ============================== BrainTemplate ==============================
        if not fileFound:
            for dataset in volumetric_dict["brain_template"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        if filepath.endswith(".nrrd"):
                            fileFound = True
                            dataset_dict = volumetric_dict["brain_template"][dataset]
                            resource_types = dataset_dict["type"]
                            voxel_type = dataset_dict["voxel_type"]
                            description = f"{dataset_dict['description']}"
                            atlasrelease_choice = dataset_dict["atlasrelease"]
                            dataSampleModality = dataset_dict["datasamplemodality"]
                            voxel_type = dataset_dict["voxel_type"]
                            dataset_name = dataset_dict["name"]
                            # this is going ot be the "name" of the resource
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            break
                        else:
                            L.error(
                                f"Error: braintemplate dataset '{filepath}' is not a "
                                "volumetric .nrrd file"
                            )
                            exit(1)
                except FileNotFoundError:
                    pass
        # ============================ CELL ORIENTATIONS ============================
        if not fileFound:
            for dataset in volumetric_dict["cell_orientations"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        if filepath.endswith(".nrrd"):
                            fileFound = True
                            dataset_dict = volumetric_dict["cell_orientations"][dataset]
                            resource_types = dataset_dict["type"]
                            voxel_type = dataset_dict["voxel_type"]
                            description = f"{dataset_dict['description']}"
                            atlasrelease_choice = dataset_dict["atlasrelease"]
                            dataSampleModality = dataset_dict["datasamplemodality"]
                            dataset_name = dataset_dict["name"]
                            if dataSampleModality == "quaternion":
                                dimension_name = "quaternion"
                            # this is going ot be the "name" of the resource
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            break
                        else:
                            L.error(
                                f"Error: parcellation dataset '{filepath}' is not a "
                                "volumetric .nrrd file"
                            )
                            exit(1)
                except FileNotFoundError:
                    pass

        # ============================== PLACEMENT HINTS ==============================
        if not fileFound:
            for dataset in volumetric_dict["placement_hints"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        fileFound = True
                        resource_flag = "isPH"
                        if os.path.isdir(filepath):
                            isFolder = True
                            directory = filepath
                            files = os.listdir(directory)
                            pattern = "*layer*"
                            files_list = fnmatch.filter(files, pattern)
                            files_list = sorted(files_list)
                            if len(files_list) != 6:
                                L.error(
                                    f"Error: '{filepath}' do not contain 6 placement "
                                    "hints volumetric files."
                                )
                                exit(1)
                            pattern = "*y.nrrd"
                            ylayer = fnmatch.filter(files, pattern)
                            if not ylayer:
                                L.error(
                                    f"Error: '{filepath}' do not contain the file "
                                    "[PH]y.nrrd volumetric files."
                                )
                                exit(1)
                            pattern = "*problematic_voxel_mask.nrrd"
                            mask_error = fnmatch.filter(files, pattern)
                            if not mask_error:
                                L.error(
                                    f"Error: '{filepath}' do not contain the "
                                    "problematic_voxel_mask file."
                                )
                                exit(1)
                            pattern = "*report.json"
                            report_error = fnmatch.filter(files, pattern)
                            if not report_error:
                                L.error(
                                    f"Error: '{filepath}' do not contain the report "
                                    "json file"
                                )
                                exit(1)
                            files_list.extend(ylayer)
                            files_list.extend(mask_error)
                            files_list.extend(report_error)
                            # this is going ot be the "name" of the resource
                            filepath = os.path.join(directory, files_list[0])
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            dataset_dict = volumetric_dict["placement_hints"][dataset]
                            resource_types = dataset_dict["type"]
                            voxel_type = dataset_dict["voxel_type"]
                            layer_number = re.findall(
                                r"\d+", filename_noext.replace("_", " ").title()
                            )
                            description = dataset_dict["description"]
                            description = description.replace("XX", layer_number[0])
                            for layer_nbr, layer_id in const.isocortex_layers.items():
                                if layer_nbr == layer_number[0]:
                                    layer = {
                                        "@id": layer_id,
                                        "label": f"layer {layer_nbr}",
                                    }
                                    brainLocation["layer"] = layer
                                    pass
                            atlasrelease_choice = dataset_dict["atlasrelease"]
                            dataSampleModality = dataset_dict["datasamplemodality"]
                            dataset_name = dataset_dict["name"]
                            suffixe = dataset_dict["suffixe"]
                            annotation_description = const.description_ccfv3_split
                            break
                        else:
                            L.error(
                                f"Error: placement hints dataset '{filepath}' is not a "
                                "folder containing placement hints volume and error "
                                "reports."
                            )
                            exit(1)

                except FileNotFoundError:
                    pass

        # =============================== Region MASKS ===============================
        if not fileFound:
            for dataset in volumetric_dict["volume_mask"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        resource_flag = "isRegionMask"
                        fileFound = True
                        if os.path.isdir(filepath):
                            isFolder = True
                            directory = filepath
                            files = os.listdir(directory)
                            pattern = "*.nrrd"
                            files_list = fnmatch.filter(files, pattern)
                            if not files_list:
                                L.error(
                                    f"Error: '{filepath}' do not contain any mask "
                                    "volumetric files."
                                )
                                exit(1)
                            # this is going ot be the "name" of the resource
                            filepath = os.path.join(directory, files_list[0])
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            dataset_dict = volumetric_dict["volume_mask"][dataset]
                            resource_types = dataset_dict["type"]
                            try:
                                region_id = int(filename_noext)
                            except ValueError as error:
                                L.error(
                                    f"ValueError in '{filepath}' file name. {error}. "
                                    "The mask file names have to be integer "
                                    "representing their region"
                                )
                                exit(1)
                            voxel_type = dataset_dict["voxel_type"]
                            try:
                                hierarchy_path = get_hierarchy_file(
                                    input_hierarchy,
                                    config_content,
                                    dataset_dict["hierarchy_tag"],
                                )
                                region_name, hierarchy_tree = get_brain_region_prop(
                                    region_id, ["name"], hierarchy_path, flat_tree
                                )
                                region_name = region_name["name"]
                                flat_tree = hierarchy_tree
                            except KeyError as e:
                                L.error(f"KeyError: {e}")
                                exit(1)
                            except ValueError as e:
                                L.error(f"ValueError: {e}")
                                exit(1)
                            L.info(
                                f"Creating the Mask payload for region {region_id}..."
                            )
                            description = (
                                f"Binary mask volume - {region_name.title()} (ID: "
                                f"{region_id}) - for the "
                                f"{dataset_dict['description']}."
                            )
                            brainLocation["brainRegion"]["@id"] = f"mba:{region_id}"
                            brainLocation["brainRegion"]["label"] = f"{region_name}"
                            atlasrelease_choice = dataset_dict["atlasrelease"]
                            dataSampleModality = dataset_dict["datasamplemodality"]
                            dataset_name = dataset_dict["name"]
                            # this is going ot be the "name" of the resource
                            suffixe = dataset_dict["suffixe"]
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            filename_noext = f"{region_name.title()} Mask {suffixe}"
                            if link_regions_path:
                                try:
                                    with open(
                                        link_regions_path, "r+"
                                    ) as link_summary_file:
                                        link_summary_file = open(
                                            link_regions_path, "r+"
                                        )
                                        link_summary_file.seek(0)
                                        link_summary_content = json.loads(
                                            link_summary_file.read()
                                        )
                                    action_summary_file = "append"
                                except json.decoder.JSONDecodeError:
                                    action_summary_file = "write"
                                except FileNotFoundError:
                                    action_summary_file = "write"
                            break

                        else:
                            L.error(
                                f"Error: volumetric mask dataset '{filepath}' is not a "
                                "folder containing binary mask volume."
                            )
                            exit(1)

                except FileNotFoundError:
                    pass

        # If still no file found at this step then raise error
        if not fileFound:
            L.error(
                f"Error: '{filepath}' does not correspond to one of the datasets "
                "defined in the VolumetricFile section of the 'generated dataset' "
                "configuration file"
            )
            exit(1)

        # Parsing the header of the NRRD file
        header = None
        try:
            header = nrrd.read_header(filepath)
        except nrrd.errors.NRRDError as e:
            L.error(f"NrrdError: {e}")
            L.info("Aborting pushing process.")  # setLevel(logging.INFO)
            exit(1)

        config = {
            "file_extension": file_extension,
            "sampling_space_unit": const.SPATIAL_UNIT,
            "sampling_period": const.default_sampling_period,
            "sampling_time_unit": const.default_sampling_time_unit,
        }
        # ==== Create/fetch the atlasRelease Resource linked to the input datasets ====
        if not isinstance(forge._store, DemoStore):
            # Check that the same atlasrelease is not treated again (need to be
            # different + not been treated yet)
            if atlasrelease_choice != atlasrelease_payloads["atlasrelease_choice"]:
                if atlasrelease_choice not in atlasrelease_choosen:
                    differentAtlasrelease = True
                else:
                    differentAtlasrelease = False
                atlasrelease_payloads["atlasrelease_choice"] = atlasrelease_choice
                try:
                    atlasrelease_payloads = return_atlasrelease(
                        forge,
                        atlasrelease_config_path,
                        atlasrelease_payloads,
                        resource_tag,
                        isSecondaryCLI=False,
                    )
                    if (
                        not atlasrelease_payloads["aibs_atlasrelease"]
                        and atlasrelease_choice not in atlasrelease_choosen
                    ):
                        if atlasrelease_payloads["fetched"]:
                            L.info(
                                f"atlasrelease Resource '{atlasrelease_choice}' found "
                                "in the Nexus destination project "
                                f"'{forge._store.bucket}'"
                            )
                        else:
                            L.info(
                                f"atlasrelease Resource '{atlasrelease_choice}' has "
                                "not been found in the Nexus destination project "
                                f"'{forge._store.bucket}'. A new one will be created "
                                "and pushed"
                            )
                    atlasrelease_choosen.append(atlasrelease_choice)
                except Exception as e:
                    L.error(f"Exception: {e}")
                    exit(1)
                except AttributeError as e:
                    L.error(f"AttributeError: {e}")
                    exit(1)
            else:
                differentAtlasrelease = False

            if isinstance(atlasrelease_payloads["atlas_release"], dict):
                atlasRelease = {
                    "@id": atlasrelease_payloads["atlas_release"]["@id"],
                    "@type": atlasrelease_payloads["atlas_release"]["@type"],
                }
            else:
                atlasRelease = {
                    "@id": atlasrelease_payloads["atlas_release"].id,
                    "@type": atlasrelease_payloads["atlas_release"].type,
                }

                # ======== Check that Ontology and Parcellation are presents ========

                # For a new atlas release creation verify first that the right
                # parcellation volume and hierarchy file have been provided and attach
                # the distribution. For an update, compare first if they distribution
                # are different before attaching it
                # => check if the good hierarchy file is given in input
                if (
                    differentAtlasrelease
                    and not atlasrelease_payloads["aibs_atlasrelease"]
                ):
                    try:
                        atlasrelease_ontology_path = get_hierarchy_file(
                            input_hierarchy,
                            config_content,
                            const.atlasrelease_dict[atlasrelease_choice]["ontology"][
                                "name"
                            ],
                        )
                    except KeyError:
                        # If the distribution is empty the good file is needed for a new
                        # creation
                        if not atlasrelease_payloads["hierarchy"].distribution:
                            L.error(
                                "Error: the ontology file corresponding to the "
                                "created atlasRelease resource can not be found among "
                                "input hierarchy files."
                            )
                            exit(1)

                    # Build the distribution dict with the input hierarchy file
                    format_hierarchy_original = os.path.splitext(
                        os.path.basename(atlasrelease_ontology_path)
                    )[1][1:]
                    content_type_original = f"application/{format_hierarchy_original}"
                    hierarchy_original_hash = return_file_hash(
                        atlasrelease_ontology_path
                    )
                    input_hierarchy_distrib = {
                        f"{content_type_original}": (
                            hierarchy_original_hash,
                            atlasrelease_ontology_path,
                        )
                    }
                    # If the correct hierarchy jsonld file is given in input then add it
                    # to the distribution dict.
                    try:
                        hierarchy_mba = const.atlasrelease_dict[atlasrelease_choice][
                            "ontology"
                        ]["mba_jsonld"]
                        if os.path.samefile(
                            input_hierarchy_jsonld,
                            hierarchies[hierarchy_mba],
                        ):
                            content_type_mba = "application/ld+json"
                            hierarchy_mba_hash = return_file_hash(
                                input_hierarchy_jsonld
                            )
                            hierarchy_mba_dict = {
                                f"{content_type_mba}": (
                                    hierarchy_mba_hash,
                                    input_hierarchy_jsonld,
                                )
                            }
                            input_hierarchy_distrib.update(hierarchy_mba_dict)
                    except FileNotFoundError as error:
                        L.error(
                            f"Error : {error}. Input hierarchy jsonLD file "
                            "does not correspond to the input hierarchy "
                            "json file"
                        )
                        exit(1)
                    # if no input hierarchy jsonLD has been provided then
                    # input_hierarchy_jsonld is None and os.path.samefile raises a
                    # TypeError
                    except TypeError:
                        pass
                    # If the hierarchy file has been fetched then the distribution will
                    # be updated with the one given in input only if it is different
                    # from the ones from the distribution dict. For a brand new file,
                    # the distribution will be attached by default.
                    distribution_ontologies = []
                    if atlasrelease_payloads["hierarchy"].distribution:
                        if not isinstance(
                            atlasrelease_payloads["hierarchy"].distribution, list
                        ):
                            atlasrelease_payloads["hierarchy"].distribution = [
                                atlasrelease_payloads["hierarchy"].distribution
                            ]
                        # Compare the fetched hierarchy file hash with the hash from
                        # the input ones
                        for fetched_distrib in atlasrelease_payloads[
                            "hierarchy"
                        ].distribution:
                            try:
                                if (
                                    fetched_distrib.digest.value
                                    != input_hierarchy_distrib[
                                        fetched_distrib.encodingFormat
                                    ][0]
                                ):
                                    distribution_hierarchy = forge.attach(
                                        input_hierarchy_distrib[
                                            fetched_distrib.encodingFormat
                                        ][1],
                                        fetched_distrib.encodingFormat,
                                    )
                                    # attach the selected input distribution and pop it
                                    # from the dictionary
                                    distribution_ontologies.append(
                                        distribution_hierarchy
                                    )
                                else:
                                    # If the distribution is the same, keep it
                                    distribution_ontologies.append(fetched_distrib)
                                input_hierarchy_distrib.pop(
                                    fetched_distrib.encodingFormat
                                )
                            # If the distribution is empty
                            except AttributeError:
                                pass
                            except KeyError:
                                pass

                        # If still keys in it then attach the remaining files
                        if input_hierarchy_distrib:
                            for encoding, file in input_hierarchy_distrib.items():
                                distribution_hierarchy = forge.attach(
                                    file[1],
                                    encoding,
                                )
                                distribution_ontologies.append(distribution_hierarchy)
                    else:
                        # If the hierarchy file is new so it does not have a
                        # distribution then attach the distribution from the input files
                        for encoding, file in input_hierarchy_distrib.items():
                            distribution_hierarchy = forge.attach(file[1], encoding)
                            distribution_ontologies.append(distribution_hierarchy)
                    atlasrelease_payloads[
                        "hierarchy"
                    ].distribution = distribution_ontologies
                    # => check if the good parcellation file is given in input right now
                    # but link it to the atlasRelease resource later during the dataset
                    # payload creation loop
                    atlasrelease_parcellation = const.atlasrelease_dict[
                        atlasrelease_choice
                    ]["parcellation"]
                    for datasetpath in inputpath:
                        try:
                            if os.path.samefile(
                                datasetpath, volumes[atlasrelease_parcellation]
                            ):
                                pass
                        except FileNotFoundError:
                            if atlasrelease_payloads[
                                "atlas_release"
                            ].parcellationVolume:
                                L.error(
                                    "Error: the parcellation file corresponding to "
                                    "the created atlasRelease resource can not be "
                                    "found among input dataset files"
                                )
                                exit(1)

                    # =================== Derivation Hierarchy file ===================
                    # Check if a dataset derive from the hierarchy file
                    hierarchy_name = const.atlasrelease_dict[atlasrelease_choice][
                        "ontology"
                    ]["name"]
                    if hierarchy_name in deriv_dict_id.keys():
                        atlasrelease_payloads["hierarchy"].id = deriv_dict_id[
                            hierarchy_name
                        ]["id"]

                    hierarchy_deriv = []
                    for deriv_key, deriv_value in deriv_dict_id.items():
                        if hierarchy_name in deriv_value["datasets"]:
                            deriv_type = []
                            for volumetric_type, content in volumetric_dict.items():
                                try:
                                    deriv_type = content[f"{volumes[deriv_key]}"][0]
                                    if deriv_type not in resource_types:
                                        deriv_type = ["Dataset", deriv_type]
                                    else:
                                        deriv_type = "Dataset"
                                except KeyError:
                                    pass
                            # if the derivation is not a known volumetric dataset then
                            # it is an ontology
                            if not deriv_type:
                                deriv_type = ["Entity", const.ontology_type]
                            deriv = {
                                "@type": "Derivation",
                                "entity": {
                                    "@id": deriv_value["id"],
                                    "@type": deriv_type,
                                },
                            }
                            hierarchy_deriv.append(deriv)
                    # If only 1 item no need for it to be a list
                    if len(hierarchy_deriv) == 1:
                        hierarchy_deriv = hierarchy_deriv[0]

                    # =================== Link atlasRelease/Ontology ===================
                    if not atlasrelease_payloads["atlas_release"].parcellationOntology:
                        atlasrelease_payloads["atlas_release"].parcellationOntology = {
                            "@id": atlasrelease_payloads["hierarchy"].id,
                            "@type": ["Entity", const.ontology_type, "Ontology"],
                        }
                    atlasrelease_payloads["hierarchy"].contribution = contribution
                    if atlasrelease_payloads["hierarchy"]._store_metadata:
                        resources_payloads["datasets_toUpdate"][
                            f"{const.schema_ontology}"
                        ].append(atlasrelease_payloads["hierarchy"])
                    else:
                        resources_payloads["datasets_toPush"][
                            f"{const.schema_ontology}"
                        ].append(atlasrelease_payloads["hierarchy"])

        # ==================== Fetch atlasRelease linked resources ====================
        if (
            not isinstance(forge._store, DemoStore)
            and not atlasrelease_payloads["aibs_atlasrelease"]
        ):
            try:
                if os.path.samefile(volumes[atlasrelease_parcellation], filepath):
                    resource_flag = "isAtlasParcellation"
                    try:
                        parcellationAtlas_id = atlasrelease_payloads[
                            "atlas_release"
                        ].parcellationVolume["@id"]
                    except AttributeError:
                        pass
            except FileNotFoundError:
                pass
            except KeyError:
                pass

        if (
            atlasrelease_payloads["fetched"]
            or atlasrelease_payloads["aibs_atlasrelease"]
        ):
            resource_type_list = list(
                set(resource_types).difference(
                    set([const.dataset_type, const.volumetric_type])
                )
            )
            datasamplemodality_list = [dataSampleModality]
            if resource_flag == "isPH":
                resource_type_list_2 = list(
                    set(
                        volumetric_dict["placement_hints"][dataset]["type_2"]
                    ).difference(set([const.dataset_type, const.volumetric_type]))
                )
                resource_type_list = [
                    resource_type_list[0],
                    resource_type_list_2[0],
                ]
                datasamplemodality_list = [
                    dataSampleModality,
                    volumetric_dict["placement_hints"][dataset]["datasamplemodality_2"],
                ]
            try:
                # fetched_resources will be either one resource or a dictionary of
                # resource
                fetched_resources = fetch_linked_resources(
                    forge,
                    atlasRelease,
                    resource_type_list,
                    datasamplemodality_list,
                    resource_flag,
                    parcellationAtlas_id=parcellationAtlas_id,
                )
            except KeyError as error:
                L.error(f"{error}")
                exit(1)
            except IndexError as error:
                L.error(f"{error}")
                exit(1)
        else:

            # ======================= Create the derivation prop =======================

            if resource_flag != "isCellDensity":
                derivation = []
                for deriv_key, deriv_value in deriv_dict_id.items():
                    if dataset_name in deriv_value["datasets"]:
                        deriv_type = []
                        # the type is given for the derivation that are
                        # 'input_dataset_used'
                        try:
                            if deriv_value["type"] == "ParcellationOntology":
                                deriv_type = ["Entity", deriv_value["type"]]
                            else:
                                deriv_type = "Dataset"
                        except KeyError:
                            pass
                        if not deriv_type:
                            for volumetric_type, content in volumetric_dict.items():
                                try:
                                    deriv_type = content[f"{volumes[deriv_key]}"][0]
                                    # In order to avoir RegistrationError due to
                                    # derivation having a type constrained by the
                                    # volumetricdatalayer schema :
                                    deriv_type = "Dataset"
                                except KeyError:
                                    pass
                        # if the derivation is not a known volumetric dataset then
                        # it is an ontology
                        if not deriv_type:
                            deriv_type = ["Entity", const.ontology_type]
                        deriv = {
                            "@type": "Derivation",
                            "entity": {
                                "@id": deriv_value["id"],
                                "@type": deriv_type,
                            },
                        }
                        derivation.append(deriv)
                # If only 1 item no need for it to be a list
                if len(derivation) == 1:
                    derivation = derivation[0]

        # ==================== add Activity and generation prop ====================
        if provenance_metadata and not activity_resource:
            try:
                activity_resource = return_activity_payload(forge, provenance_metadata)
                if not activity_resource._store_metadata:
                    L.info(
                        "Existing activity resource not found in the Nexus destination "
                        f"project '{forge._store.bucket}'. A new activity will be "
                        "created and registered"
                    )
            except Exception as e:
                L.error(f"{e}")
                exit(1)

            # if the activity Resource has been fetched from Nexus, the property
            # 'value' need to be mapped back to @value
            if hasattr(activity_resource, "startedAtTime"):
                # A Resource property that is a dict at the creation of the Resource
                # becomes a Resource attribut after being synchronized on Nexus
                if not isinstance(activity_resource.startedAtTime, dict):
                    if hasattr(activity_resource.startedAtTime, "@value") and hasattr(
                        activity_resource.startedAtTime, "type"
                    ):
                        value = getattr(activity_resource.startedAtTime, "@value")
                        type = getattr(activity_resource.startedAtTime, "type")
                        activity_resource.startedAtTime = forge.from_json(
                            {
                                "@type": type,
                                "@value": value,
                            }
                        )

            generation = {
                "@type": "Generation",
                "activity": {
                    "@id": activity_resource.id,
                    "@type": activity_resource.type,
                },
            }

        # =========================== 1st Payload creation ===========================

        # We create a 1st payload that will be recycled in case of multiple files to
        # push

        name = filename_noext.replace("_", " ").title()

        # If the resource has been fetched, we compare its distribution to the input
        # file, copy its id and _store_metadata
        if fetched_resources:
            filepath_hash = return_file_hash(filepath)
            # If the fetched resources are a dict full of resource per regions
            first_fetched_resource = None
            if isinstance(fetched_resources, dict):
                if resource_flag == "isRegionMask":
                    try:
                        first_fetched_resource = fetched_resources[f"{region_id}"]
                    # If the region in particular is not found then do not update this
                    # one and create it instead
                    except KeyError:
                        content_type = f"application/{file_extension}"
                        distribution_file = forge.attach(filepath, content_type)
                        pass
                if resource_flag == "isPH":
                    try:
                        first_fetched_resource = fetched_resources[f"{layer_number[0]}"]
                    # If the region in particular is not found then do not update this
                    # one and create it instead
                    except KeyError:
                        content_type = f"application/{file_extension}"
                        distribution_file = forge.attach(filepath, content_type)
                        pass
            else:
                first_fetched_resource = fetched_resources
            if first_fetched_resource:
                toUpdate = True
                fetched_resource_id = first_fetched_resource.id
                fetched_resource_metadata = first_fetched_resource._store_metadata
                try:
                    if (
                        filepath_hash
                        != first_fetched_resource.distribution.digest.value
                    ):
                        content_type = f"application/{file_extension}"
                        distribution_file = forge.attach(filepath, content_type)
                    else:
                        distribution_file = first_fetched_resource.distribution
                # If no distribution in the fetched resources then attach the input file
                except AttributeError:
                    content_type = f"application/{file_extension}"
                    distribution_file = forge.attach(filepath, content_type)
            else:
                content_type = f"application/{file_extension}"
                distribution_file = forge.attach(filepath, content_type)
        else:
            content_type = f"application/{file_extension}"
            distribution_file = forge.attach(filepath, content_type)
        nrrd_resource = Resource(
            type=resource_types,
            name=name,
            distribution=distribution_file,
            description=description,
            isRegisteredIn=const.isRegisteredIn,
            brainLocation=brainLocation,
            atlasRelease=atlasRelease,
            dataSampleModality=dataSampleModality,
            subject=const.subject,
            contribution=contribution,
        )

        nrrd_resource = add_nrrd_props(nrrd_resource, header, config, voxel_type)

        if derivation:
            nrrd_resource.derivation = derivation

        if fetched_resource_id:
            nrrd_resource.id = fetched_resource_id
        elif dataset in dict_ids:
            nrrd_resource.id = dict_ids[dataset]
        elif dataset_name in deriv_dict_id.keys():
            nrrd_resource.id = deriv_dict_id[dataset_name]["id"]

        if dimension_name:
            nrrd_resource.dimension[0]["name"] = dimension_name

        if generation:
            nrrd_resource.generation = generation

        if resource_flag == "isPH":
            nrrd_resource.name = f"{nrrd_resource.name} {suffixe}"

        if fetched_resource_metadata:
            nrrd_resource._store_metadata = fetched_resource_metadata

        if action_summary_file:
            if hasattr(nrrd_resource, "id"):
                mask_id = nrrd_resource.id
            else:
                mask_id = forge.format(
                    "identifier", "volumetricdatalayer", str(uuid4())
                )
                nrrd_resource.id = mask_id
            mask_link = {"mask": {"@id": mask_id}}
            if action_summary_file == "append":
                try:
                    if "mask" not in link_summary_content[f"{region_id}"].keys():
                        link_summary_content[f"{region_id}"].update(mask_link)
                    else:
                        link_summary_content[f"{region_id}"] = mask_link
                except KeyError:
                    link_summary_content[f"{region_id}"] = mask_link
            else:
                region_summary = {
                    f"{region_id}": {
                        "mask": {"@id": mask_id},
                    }
                }
                link_summary_content.update(region_summary)

        # ====================== Link atlasRelease/Parcellation ======================

        if (
            resource_flag == "isAtlasParcellation"
            and not atlasrelease_payloads["aibs_atlasrelease"]
        ):
            if not atlasrelease_payloads["atlas_release"].parcellationVolume:
                if not hasattr(nrrd_resource, "id"):
                    nrrd_resource.id = forge.format(
                        "identifier", "volumetricdatalayer", str(uuid4())
                    )
                atlasrelease_payloads["atlas_release"].parcellationVolume = {
                    "@id": nrrd_resource.id,
                    "@type": ["Dataset", "BrainParcellationDataLayer"],
                }
            atlasrelease_payloads["atlas_release"].contribution = contribution

        # Add the generation prop for every different atlasRelease
        if differentAtlasrelease and not atlasrelease_payloads["aibs_atlasrelease"]:
            if generation:
                atlasrelease_payloads["hierarchy"].generation = generation
                atlasrelease_payloads["atlas_release"].generation = generation
            if atlasrelease_payloads["fetched"]:
                resources_payloads["datasets_toUpdate"][
                    f"{const.schema_atlasrelease}"
                ].append(atlasrelease_payloads["atlas_release"])
            else:
                resources_payloads["datasets_toPush"][
                    f"{const.schema_atlasrelease}"
                ].append(atlasrelease_payloads["atlas_release"])

        if toUpdate:
            resources_payloads["datasets_toUpdate"][
                f"{const.schema_volumetricdatalayer}"
            ].append(nrrd_resource)
        else:
            resources_payloads["datasets_toPush"][
                f"{const.schema_volumetricdatalayer}"
            ].append(nrrd_resource)
        # =========================== Directory Datasets ===========================

        # If the input is a folder containing several dataset to push
        if isFolder:
            for f in range(1, len(files_list)):  # start at the 2nd file
                toUpdate = False
                fetched_resource_id = None
                fetched_resource_metadata = None
                brainLocation = {
                    "brainRegion": {
                        "@id": "mba:997",
                        "label": "root",
                    },
                    "atlasSpatialReferenceSystem": {
                        "@type": [
                            "BrainAtlasSpatialReferenceSystem",
                            "AtlasSpatialReferenceSystem",
                        ],
                        "@id": const.atlas_spatial_reference_system_id,
                    },
                }
                filepath = os.path.join(directory, files_list[f])
                filename_noext = os.path.splitext(os.path.basename(filepath))[0]
                file_extension = os.path.splitext(os.path.basename(filepath))[1][1:]
                name = filename_noext.replace("_", " ").title()

                # ============ CELL DENSITY = TO REWORK LIKE THE OTHERS ============
                if resource_flag == "isCellDensity":
                    file_split = filename_noext.split("_")
                    content_type = f"application/{file_extension}"
                    distribution_file = forge.attach(filepath, content_type)
                    v = "mtypes_densities_probability_map_ccfv2_correctednissl"
                    try:
                        if os.path.samefile(
                            directory,
                            volumes["mtypes_densities_profile_ccfv2_correctednissl"],
                        ) or os.path.samefile(
                            directory,
                            volumes[v],
                        ):
                            file_split.insert(0, "Mtype")
                    except FileNotFoundError:
                        pass
                    cell_density_file = " ".join(file_split)
                    dataset_dict = volumetric_dict["cell_densities"][dataset]
                    description = (
                        f"{cell_density_file[0].upper()}{cell_density_file[1:]} volume "
                        f"for the {dataset_dict['description']}."
                    )
                    name = cell_density_file.title()
                    if atlasrelease_choice == "atlasrelease_ccfv2":
                        name = f"{name} Ccfv2 Corrected Nissl"

                if resource_flag == "isPH":
                    # Do not create specific payload for the json report because it is
                    # included with the problematic mask
                    if f >= 8:
                        break
                    name = f"{name} {suffixe}"
                    if f <= 5:
                        layer_number = re.findall(r"\d+", files_list[f])
                        description = dataset_dict["description"]
                        description = description.replace("XX", layer_number[0])
                        for layer_nbr, layer_id in const.isocortex_layers.items():
                            if layer_nbr == layer_number[0]:
                                layer = {
                                    "@id": layer_id,
                                    "label": f"layer {layer_nbr}",
                                }
                                brainLocation["layer"] = layer
                                pass
                    if f == 6:
                        description = (
                            "Volume containing for each voxel its distance from the "
                            f"bottom of the {annotation_description} Isocortex. The "
                            "bottom being the deepest part of the Isocortex (highest "
                            "cortical depth)."
                        )
                        layer_number = re.findall(r"\d+", files_list[f])
                    if fetched_resources:
                        try:
                            fetched_resource_id = fetched_resources[
                                f"{layer_number[0]}"
                            ].id
                            fetched_resource_metadata = fetched_resources[
                                f"{layer_number[0]}"
                            ]._store_metadata
                            toUpdate = True
                            filepath_hash = return_file_hash(filepath)
                            try:
                                if (
                                    filepath_hash
                                    != fetched_resources[
                                        f"{layer_number[0]}"
                                    ].distribution.digest.value
                                ):
                                    content_type = f"application/{file_extension}"
                                    distribution_file = forge.attach(
                                        filepath, content_type
                                    )
                                else:
                                    distribution_file = fetched_resources[
                                        f"{layer_number[0]}"
                                    ].distribution
                            except AttributeError:
                                content_type = f"application/{file_extension}"
                                distribution_file = forge.attach(filepath, content_type)
                        except KeyError:
                            toUpdate = False
                            content_type = f"application/{file_extension}"
                            distribution_file = forge.attach(filepath, content_type)
                        except IndexError:
                            toUpdate = False
                            content_type = f"application/{file_extension}"
                            distribution_file = forge.attach(filepath, content_type)
                    else:
                        content_type = f"application/{file_extension}"
                        distribution_file = forge.attach(filepath, content_type)
                    if f == 7:
                        description = (
                            f"3D mask volume of the {annotation_description}. It "
                            "includes Isocortex problematic voxels along with the "
                            "distance JSON report containing the proportion of "
                            "problematic voxels. Problematic voxels are highlighted "
                            "during the placement-hints computation and correspond to "
                            "voxels with at least one distance-related problem (i.e "
                            "those who do not intersect with the bottom or top mesh, "
                            "those with a distance gap greater than the maximum "
                            "thickness...)."
                        )
                if resource_flag == "isRegionMask":
                    try:
                        region_id = int(filename_noext)
                    except ValueError as error:
                        L.error(
                            f"ValueError in '{filepath}' file name. {error}. "
                            "The mask file names have to be integer "
                            "representing their region"
                        )
                        exit(1)
                    try:
                        region_name, hierarchy_tree = get_brain_region_prop(
                            region_id, ["name"], hierarchy_path, flat_tree
                        )
                        region_name = region_name["name"]
                    except KeyError as e:
                        L.error(f"KeyError: {e}")
                        exit(1)
                    L.info(f"Creating the Mask payload for region {region_id}...")
                    description = (
                        f"Binary mask volume - {region_name.title()} (ID: "
                        f"{region_id}) - for the "
                        f"{dataset_dict['description']}."
                    )
                    name = f"{region_name.title()} Mask {suffixe}"
                    brainLocation["brainRegion"]["@id"] = f"mba:{region_id}"
                    brainLocation["brainRegion"]["label"] = f"{region_name}"
                    if fetched_resources:
                        try:
                            fetched_resource_id = fetched_resources[f"{region_id}"].id
                            fetched_resource_metadata = fetched_resources[
                                f"{region_id}"
                            ]._store_metadata
                            toUpdate = True
                            filepath_hash = return_file_hash(filepath)
                            try:
                                if (
                                    filepath_hash
                                    != fetched_resources[
                                        f"{region_id}"
                                    ].distribution.digest.value
                                ):
                                    content_type = f"application/{file_extension}"
                                    distribution_file = forge.attach(
                                        filepath, content_type
                                    )
                                else:
                                    distribution_file = fetched_resources[
                                        f"{region_id}"
                                    ].distribution
                            except AttributeError:
                                content_type = f"application/{file_extension}"
                                distribution_file = forge.attach(filepath, content_type)
                        except KeyError:
                            toUpdate = False
                            content_type = f"application/{file_extension}"
                            distribution_file = forge.attach(filepath, content_type)
                    else:
                        content_type = f"application/{file_extension}"
                        distribution_file = forge.attach(filepath, content_type)

                # Use forge.reshape instead ?
                nrrd_resources = Resource(
                    type=nrrd_resource.type,
                    name=name,
                    distribution=distribution_file,
                    description=description,
                    contribution=nrrd_resource.contribution,
                    isRegisteredIn=nrrd_resource.isRegisteredIn,
                    brainLocation=brainLocation,
                    atlasRelease=nrrd_resource.atlasRelease,
                    componentEncoding=nrrd_resource.componentEncoding,
                    fileExtension=nrrd_resource.fileExtension,
                    dimension=nrrd_resource.dimension,
                    sampleType=nrrd_resource.sampleType,
                    worldMatrix=nrrd_resource.worldMatrix,
                    resolution=nrrd_resource.resolution,
                    bufferEncoding=nrrd_resource.bufferEncoding,
                    endianness=nrrd_resource.endianness,
                    dataSampleModality=nrrd_resource.dataSampleModality,
                    subject=nrrd_resource.subject,
                )

                if resource_flag == "isPH":
                    if 5 < f < 8:
                        try:
                            header = nrrd.read_header(filepath)
                        except nrrd.errors.NRRDError as e:
                            L.error(f"NrrdError: {e}")
                            L.info("Aborting pushing process.")
                            exit(1)
                        config["file_extension"] = os.path.splitext(
                            os.path.basename(files_list[f])
                        )[1][1:]
                        voxel_type = dataset_dict["voxel_type_2"]
                        nrrd_resources = add_nrrd_props(
                            nrrd_resources, header, config, voxel_type
                        )
                    if f >= 7:
                        nrrd_resources.dataSampleModality = dataset_dict[
                            "datasamplemodality_2"
                        ]
                        nrrd_resources.type = dataset_dict["type_2"]
                        # Compare and attach the multiple distributions
                        report_json = os.path.join(directory, files_list[8])
                        format_json = os.path.splitext(os.path.basename(report_json))[
                            1
                        ][1:]
                        report_json_hash = return_file_hash(report_json)
                        filepath_hash = return_file_hash(filepath)
                        report_content_type = f"application/{format_json}"
                        ph_report_distrib = {
                            f"{report_content_type}": (report_json_hash, report_json),
                            f"{content_type}": (filepath_hash, filepath),
                        }
                        distribution_file = []
                        if fetched_resources:
                            try:
                                fetched_resource_id = fetched_resources["report"].id
                                fetched_resource_metadata = fetched_resources[
                                    "report"
                                ]._store_metadata
                                toUpdate = True
                                if fetched_resources["report"].distribution:
                                    if not isinstance(
                                        fetched_resources["report"].distribution, list
                                    ):
                                        fetched_resources["report"].distribution = [
                                            fetched_resources["report"].distribution
                                        ]
                                for fetched_distrib in fetched_resources[
                                    "report"
                                ].distribution:
                                    try:
                                        if (
                                            fetched_distrib.digest.value
                                            != ph_report_distrib[
                                                fetched_distrib.encodingFormat
                                            ][0]
                                        ):
                                            distribution_report = forge.attach(
                                                ph_report_distrib[
                                                    fetched_distrib.encodingFormat
                                                ][1],
                                                fetched_distrib.encodingFormat,
                                            )
                                            distribution_file.append(
                                                distribution_report
                                            )
                                        else:
                                            # If the distribution is the same, keep it
                                            distribution_file.append(fetched_distrib)
                                        ph_report_distrib.pop(
                                            fetched_distrib.encodingFormat
                                        )
                                    # If the distribution is empty
                                    except AttributeError:
                                        pass
                                    except KeyError:
                                        pass
                                # If still keys in it then attach the remaining
                                # files
                                if ph_report_distrib:
                                    for encoding, file in ph_report_distrib.items():
                                        distribution_report = forge.attach(
                                            file[1],
                                            encoding,
                                        )
                                        distribution_file.append(distribution_report)
                            except KeyError:
                                toUpdate = False
                                for encoding, file in ph_report_distrib.items():
                                    distribution_report = forge.attach(
                                        file[1], encoding
                                    )
                                    distribution_file.append(distribution_report)
                        else:
                            for encoding, file in ph_report_distrib.items():
                                distribution_report = forge.attach(file[1], encoding)
                                distribution_file.append(distribution_report)
                        nrrd_resources.distribution = distribution_file

                if derivation:
                    nrrd_resources.derivation = nrrd_resource.derivation

                if fetched_resource_id:
                    nrrd_resources.id = fetched_resource_id
                elif dataset_name in deriv_dict_id.keys():
                    nrrd_resource.id = deriv_dict_id[dataset_name]["id"]

                if fetched_resource_metadata:
                    nrrd_resources._store_metadata = fetched_resource_metadata

                if generation:
                    nrrd_resources.generation = nrrd_resource.generation

                if resource_flag == "isRegionMask":
                    # Finish to fill the link_region_path file with the masks @ids
                    if action_summary_file:
                        if hasattr(nrrd_resources, "id"):
                            mask_id = nrrd_resources.id
                        else:
                            mask_id = forge.format(
                                "identifier", "volumetricdatalayer", str(uuid4())
                            )
                            nrrd_resources.id = mask_id
                        mask_link = {"mask": {"@id": mask_id}}
                        if action_summary_file == "append":
                            try:
                                if (
                                    "mask"
                                    not in link_summary_content[f"{region_id}"].keys()
                                ):
                                    link_summary_content[f"{region_id}"].update(
                                        mask_link
                                    )
                                else:
                                    link_summary_content[f"{region_id}"] = mask_link
                            except KeyError:
                                link_summary_content[f"{region_id}"] = mask_link
                        else:
                            region_summary = {
                                f"{region_id}": {
                                    "mask": {"@id": mask_id},
                                }
                            }
                            link_summary_content.update(region_summary)
                    if f == len(files_list) - 1:
                        link_summary_file = open(link_regions_path, "w")
                        link_summary_file.write(
                            json.dumps(
                                link_summary_content, ensure_ascii=False, indent=2
                            )
                        )
                        link_summary_file.close()

                if toUpdate:
                    resources_payloads["datasets_toUpdate"][
                        f"{const.schema_volumetricdatalayer}"
                    ].append(nrrd_resources)
                else:
                    resources_payloads["datasets_toPush"][
                        f"{const.schema_volumetricdatalayer}"
                    ].append(nrrd_resources)

    resources_payloads["tag"] = atlasrelease_payloads["tag"]
    resources_payloads["activity"] = activity_resource

    # Annotate the atlasrelease_config json file with the atlasrelease "id" and "tag"
    # TODO Turn it into a function annotate_atlasrelease_file
    if (
        not isinstance(forge._store, DemoStore)
        and not atlasrelease_payloads["aibs_atlasrelease"]
    ):
        if atlasrelease_config_path:
            atlasrelease_id = atlasrelease_payloads["atlas_release"].id
            atlasrelease_link = {
                f"{atlasrelease_choice}": {
                    "id": atlasrelease_id,
                    "tag": atlasrelease_payloads["tag"],
                }
            }
            try:
                with open(atlasrelease_config_path) as atlasrelease_config_file:
                    atlasrelease_config_content = json.loads(
                        atlasrelease_config_file.read()
                    )
                    if atlasrelease_choice in atlasrelease_config_content.keys():
                        atlasrelease_config_content[
                            f"{atlasrelease_choice}"
                        ] = atlasrelease_link[f"{atlasrelease_choice}"]
                    else:
                        atlasrelease_config_content.update(atlasrelease_link)
                with open(atlasrelease_config_path, "w") as atlasrelease_config_file:
                    atlasrelease_config_file.write(
                        json.dumps(
                            atlasrelease_config_content, ensure_ascii=False, indent=2
                        )
                    )
            except FileNotFoundError:
                with open(atlasrelease_config_path, "w") as atlasrelease_config_file:
                    atlasrelease_config_file.write(
                        json.dumps(atlasrelease_link, ensure_ascii=False, indent=2)
                    )
            except json.decoder.JSONDecodeError as error:
                L.error(f"{error} when opening the input atlasrelease json file.")
                exit(1)

    return resources_payloads


def create_deriv_id(forge, deriv_dict_id, val_dataset, key_dataset, dataset_type):
    """
    Parameters:
        forge : Instantiated and configured forge object.
        deriv_dict_id : Dict containing the link derivation-dataset to dataset.
                        dataset @ids are missings.
        val_dataset : deriv_dict_id values corresponding to the dataset names.
        key_dataset : deriv_dict_id keys corresponding to the derivation dataset names.
        dataset_type : Dict containing all the ontology datasets from the input config
                    file.

    Returns:
        deriv_dict_id : Dict containing the link derivation-dataset to dataset with
                        their @ids.
    """
    if isinstance(val_dataset, list):
        for val in val_dataset:
            id_val = forge.format("identifier", dataset_type, str(uuid4()))
            if val not in deriv_dict_id.keys():
                deriv_dict_id[val] = {
                    "id": f"{id_val}",
                    "datasets": [key_dataset],
                }
            else:
                deriv_dict_id[val]["datasets"].append(key_dataset)
    else:
        if val_dataset not in deriv_dict_id.keys():
            id_val = forge.format("identifier", dataset_type, str(uuid4()))
            deriv_dict_id[val_dataset] = {
                "id": f"{id_val}",
                "datasets": [key_dataset],
            }
        else:
            deriv_dict_id[val_dataset]["datasets"].append(key_dataset)

    return deriv_dict_id


def create_deriv_dict_id(forge, inputpath, provenance_metadata, volumes, hierarchies):
    """
    Parameters:
        forge : Instantiated and configured forge object.
        inputpath : Input datasets paths. These datasets are either volumetric files
                    or folder containing volumetric files.
        provenance_metadata : Dict containing the provenance informations.
        volumes : Dict containing all the volumetric datasets from the input config
                file.
        hierarchies : Dict containing all the ontology datasets from the input config
                    file.

    Returns:
        deriv_dict_id : Dictionary of the form :
                {
                derivation_name1 : { "id" : "{id_value}"
                                    "datasets" : [{derivative_dataset}]
                                    }
                derivation_name2 : { "id" : "{id_value}"
                                    "datasets" : [{derivative_dataset}]
                                    }
                }
    """
    deriv_dict_id = {}
    try:
        for key_dataset, val_dataset in provenance_metadata["derivations"].items():
            derivation_found = False
            try:
                # first check if the derivation is in input_dataset_used
                if isinstance(val_dataset, list):
                    for val in val_dataset:
                        if val in provenance_metadata["input_dataset_used"].keys():
                            derivation_found = True
                            val_dataset_id = provenance_metadata["input_dataset_used"][
                                val_dataset
                            ]["id"]
                            val_dataset_type = provenance_metadata[
                                "input_dataset_used"
                            ][val_dataset]["type"]
                            if val_dataset not in deriv_dict_id.keys():
                                deriv_dict_id[val_dataset] = {
                                    "id": f"{val_dataset_id}",
                                    "datasets": [key_dataset],
                                    "type": val_dataset_type,
                                }
                        else:
                            deriv_dict_id[val_dataset]["datasets"].append(key_dataset)
                elif val_dataset in provenance_metadata["input_dataset_used"].keys():
                    derivation_found = True
                    val_dataset_id = provenance_metadata["input_dataset_used"][
                        val_dataset
                    ]["id"]
                    val_dataset_type = provenance_metadata["input_dataset_used"][
                        val_dataset
                    ]["type"]
                    if val_dataset not in deriv_dict_id.keys():
                        deriv_dict_id[val_dataset] = {
                            "id": f"{val_dataset_id}",
                            "datasets": [key_dataset],
                            "type": val_dataset_type,
                        }
                    else:
                        deriv_dict_id[val_dataset]["datasets"].append(key_dataset)
                # Then search if the dataset is part of the volumetric datasets
                if not derivation_found:
                    for dataset in volumes.keys():
                        if dataset == key_dataset:
                            derivation_found = True
                            if volumes[val_dataset] not in inputpath:
                                L.error(
                                    f"Error: The derivation dataset '{val_dataset}' "
                                    "correspond to a volumetric dataset from the input "
                                    "configuration json but the file has not been "
                                    "found among the input 'dataset-path'"
                                )
                                exit(1)
                            deriv_dict_id = create_deriv_id(
                                forge,
                                deriv_dict_id,
                                val_dataset,
                                key_dataset,
                                "volumetricdatalayer",
                            )
                    # Lastly search if the dataset is a hierarchy file
                    if not derivation_found:
                        for dataset in hierarchies.keys():
                            if dataset == key_dataset:
                                derivation_found = True
                                if hierarchies[val_dataset] not in inputpath:
                                    L.error(
                                        f"Error: The derivation dataset "
                                        f"'{val_dataset}' correspond to a hierarchy "
                                        "dataset from the input configuration json but "
                                        "the file has not been found among the input "
                                        "'hierarchy-path'"
                                    )
                                    exit(1)
                                deriv_dict_id = create_deriv_id(
                                    forge,
                                    deriv_dict_id,
                                    val_dataset,
                                    key_dataset,
                                    "ontologies",
                                )
            except KeyError as error:
                L.error(
                    f"KeyError: {error} derivation dataset is not found in the dataset "
                    "configuration file."
                )
                exit(1)
            if not derivation_found:
                L.error(
                    "Error: The derivation dataset does not match a "
                    "'input_dataset_used' dataset nor a volumetric dataset nor a "
                    "hierarchy dataset from the input dataset configuration file."
                )
                exit(1)
    except KeyError as error:
        L.error(f"The input provenance file does not contain a {error} section.")
        exit(1)

    return deriv_dict_id


def add_nrrd_props(resource, nrrd_header, config, voxel_type):
    """
    Add to the resource all the fields expected for a VolumetricDataLayer/NdRaster
    that can be found in the NRRD header. A resource dictionary must exist and be
    provided (even if empty).

    Parameters:
        resource : Resource object defined by a properties payload linked to a file.
        nrrd_header : Dict containing the input file header fields  and their
                    corresponding value.
        config : Dict containing the file extension and its sampling informations.
        voxel_type : String indicating the type of voxel contained in the volumetric
                     dataset.

    Returns:
        dataset : Resource object with all Nrrd properties added.
    """

    NRRD_TYPES_TO_NUMPY = {
        "signed char": "int8",
        "int8": "int8",
        "int8_t": "int8",
        "uchar": "uint8",
        "unsigned char": "uint8",
        "uint8": "uint8",
        "uint8_t": "uint8",
        "short": "int16",
        "short int": "int16",
        "signed short": "int16",
        "signed short int": "int16",
        "int16": "int16",
        "int16_t": "int16",
        "ushort": "int16",
        "unsigned short": "uint16",
        "unsigned short int": "uint16",
        "uint16": "uint16",
        "uint16_t": "uint16",
        "int": "int32",
        "signed int": "int32",
        "int32": "int32",
        "int32_t": "int32",
        "uint": "uint32",
        "unsigned int": "uint32",
        "uint32": "uint32",
        "uint32_t": "uint32",
        "longlong": "int64",
        "long long": "int64",
        "long long int": "int64",
        "signed long long": "int64",
        "signed long long int": "int64",
        "int64": "int64",
        "int64_t": "int64",
        "ulonglong": "uint64",
        "unsigned long long": "uint64",
        "unsigned long long int": "uint64",
        "uint64": "uint64",
        "uint64_t": "uint64",
        "float": "float32",
        "double": "float64",
    }

    space_origin = None
    if "space origin" in nrrd_header:
        space_origin = nrrd_header["space origin"].tolist()
    else:
        if nrrd_header["dimension"] == 2:
            space_origin = [0.0, 0.0]
        elif nrrd_header["dimension"] == 3:
            space_origin = [0.0, 0.0, 0.0]

    space_directions = None
    if "space directions" in nrrd_header:
        # replace the nan that pynrrd adds to None (just like in NRRD spec)
        space_directions = []
        for col in nrrd_header["space directions"].tolist():
            if np.isnan(col).any():
                space_directions.append(None)
            else:
                space_directions.append(col)

    # Here, 'space directions' being missing in the file, we hardcode an identity matrix
    # If we have 4 dimensions, we say
    else:
        if nrrd_header["dimension"] == 2:
            space_directions = [[1, 0], [0, 1]]
        elif nrrd_header["dimension"] == 3:
            space_directions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
        elif nrrd_header["dimension"] == 4:
            # the following is a very lousy way to determine if among the 4 dims,
            # or the first is components or the last is time...
            if nrrd_header["sizes"][0] < (np.mean(nrrd_header["sizes"] * 0.20)):
                space_directions = [None, [1, 0, 0], [0, 1, 0], [0, 0, 1]]  # component
            else:
                space_directions = [[1, 0, 0], [0, 1, 0], [0, 0, 1], None]  # time

        elif nrrd_header["dimension"] == 5:
            space_directions = [None, [1, 0, 0], [0, 1, 0], [0, 0, 1], None]

    resource.componentEncoding = NRRD_TYPES_TO_NUMPY[nrrd_header["type"]]
    # in case the nrrd file corresponds to a mask
    try:
        resource.endianness = nrrd_header["endian"]
    except KeyError:
        resource.endianness = "little"
    resource.bufferEncoding = nrrd_header["encoding"]
    resource.fileExtension = config["file_extension"]
    resource.dimension = []

    component_dim_index = -1
    passed_spatial_dim = False
    # for each dimension
    for i in range(0, nrrd_header["dimension"]):
        current_dim = {}
        current_dim["size"] = nrrd_header["sizes"][i].item()

        # this is a spatial dim
        if space_directions[i]:
            passed_spatial_dim = True
            current_dim["@type"] = "SpaceDimension"
            current_dim["unitCode"] = config["sampling_space_unit"]

        # this can be a component or a time dim
        else:
            # this is a time dim as it is located after space dim)
            if passed_spatial_dim:
                current_dim["@type"] = "TimeDimension"
                current_dim["samplingPeriod"] = config["sampling_period"]
                current_dim["unitCode"] = config["sampling_time_unit"]

            # this is a component dim as it is located before space dim
            else:
                # decide of the label
                component_dim_index = i
                current_dim["@type"] = "ComponentDimension"
                # current_dim["name"] = default_sample_type_multiple_components if
                # current_dim["size"] > 1 else default_sample_type_single_component
                try:
                    current_dim["name"] = get_voxel_type(
                        voxel_type, current_dim["size"]
                    )
                except ValueError as e:
                    L.error(f"ValueError: {e}")
                    exit(1)
                except KeyError as e:
                    L.error(f"KeyError: {e}")
                    exit(1)

        resource.dimension.append(current_dim)

    # repeating the name of the component dimension in the "sampleType" base level prop
    if component_dim_index >= 0:
        resource.sampleType = resource.dimension[component_dim_index]["name"]

    # As no component dim was mentioned in metadata, it means the component is of size 1
    else:
        # prepend a dimension component
        try:
            name = get_voxel_type(voxel_type, 1)
        except ValueError as e:
            L.error(f"ValueError: {e}")
            exit(1)
        component_dim = {"@type": "ComponentDimension", "size": 1, "name": name}
        resource.dimension.insert(0, component_dim)

        resource.sampleType = component_dim["name"]

    # creating the world matrix (column major)
    # 1. pynrrd creates a [nan, nan, nan] line for each 'space directions' that is
    # 'none' in the header.
    # We have to strip them off.
    worldMatrix = None
    r = []  # rotation mat
    o = space_origin
    for col in space_directions:
        if col is not None:
            r.append(col)

    # if 3D, we create a 4x4 homogeneous transformation matrix
    if len(r) == 3:
        worldMatrix = [
            r[0][0],
            r[0][1],
            r[0][2],
            0,
            r[1][0],
            r[1][1],
            r[1][2],
            0,
            r[2][0],
            r[2][1],
            r[2][2],
            0,
            o[0],
            o[1],
            o[2],
            1,
        ]

    # if 2D, we create a 3x3 homogeneous transformation matrix
    if len(r) == 2:
        worldMatrix = [r[0][0], r[0][1], 0, r[1][0], r[1][1], 0, o[0], o[1], 1]

    # nesting the matrix values into object with @value props
    for i in range(0, len(worldMatrix)):
        # worldMatrix[i] = {"@value": float(worldMatrix[i])}
        worldMatrix[i] = float(worldMatrix[i])

    resource.worldMatrix = worldMatrix

    resource.resolution = {"value": r[0][0], "unitCode": config["sampling_space_unit"]}

    return resource
