"""
Create a 'VolumetricDataLayer' resource payload to push into Nexus. This script has 
been designed to function with volumetric files generated by the Atlas pipeline.
To know more about 'VolumetricDataLayer' resources and Nexus, see 
https://bluebrainnexus.io.
Link to BBP Atlas pipeline confluence documentation: 
https://bbpteam.epfl.ch/project/spaces/x/rS22Ag
"""

import os
import yaml
import json
import numpy as np
import nrrd
import fnmatch
import re
from uuid import uuid4
import copy
from kgforge.core import Resource
from kgforge.specializations.stores.demo_store import DemoStore
from kgforge.core.commons.exceptions import RetrievalError

from bba_data_push.commons import (
    get_voxel_type,
    get_brain_region_prop,
    return_contribution,
    return_atlasrelease,
    return_activity_payload,
)
from bba_data_push.logging import create_log_handler

L = create_log_handler(__name__, "./push_nrrd_volumetricdatalayer.log")


def create_volumetric_resources(
    forge,
    inputpath: list,
    voxels_resolution: int,
    config_path,
    atlasrelease_id,
    input_hierarchy,
    link_regions_path,
    provenance_metadata_path,
    verbose,
) -> list:
    """
    Construct the input volumetric dataset property payloads that will be push with
    the corresponding files into Nexus as a resource.

    Parameters:
        forge : instantiated and configured forge object.
        inputpath : input datasets paths. These datasets are either volumetric files
                    or folder containing volumetric files.
        voxels_resolution : voxel resolution value.
        config_path : configuration yaml file path containing the names and paths of
                      the atlas-pipeline generated datasets.

    Returns:
        dataset : list containing as much Resource object as input datasets. Each
        Resource is defined by an attached input file and its properties described in
        a payload.
    """
    L.setLevel(verbose)

    config_file = open(config_path)
    config_content = yaml.safe_load(config_file.read().strip())
    config_file.close()
    try:
        volumes = config_content["GeneratedDatasetPath"]["VolumetricFile"]
    except KeyError as error:
        L.error(f"KeyError: {error} is not found in the dataset configuration file.")
        exit(1)

    if provenance_metadata_path:
        try:
            with open(provenance_metadata_path, "r") as f:
                provenance_metadata = json.loads(f.read())
        except ValueError as error:
            L.error(f"{error} : {provenance_metadata_path}.")
            exit(1)
    else:
        provenance_metadata = None

    # Builds a dictionary with the form :
    # {
    #     derivation_name1 : { "id" : "{id_value}"
    #                        "datasets" : [{derivative_dataset}]
    #                        }
    #     derivation_name2 : { "id" : "{id_value}"
    #                        "datasets" : [{derivative_dataset}]
    #                        }
    #     }
    deriv_dict_id = {}
    if provenance_metadata:
        try:
            for dataset in volumes.keys():
                for key_dataset, val_dataset in provenance_metadata[
                    "derivations"
                ].items():
                    if key_dataset == dataset:
                        if isinstance(val_dataset, list):
                            for val in val_dataset:
                                id_val = forge.format(
                                    "identifier", "volumetricdatalayer", str(uuid4())
                                )
                                if val not in deriv_dict_id.keys():
                                    deriv_dict_id[val] = {
                                        "id": f"{id_val}",
                                        "datasets": [key_dataset],
                                    }
                                else:
                                    deriv_dict_id[val]["datasets"].append(key_dataset)
                        else:
                            if val_dataset not in deriv_dict_id.keys():
                                id_val = forge.format(
                                    "identifier", "volumetricdatalayer", str(uuid4())
                                )
                                deriv_dict_id[val_dataset] = {
                                    "id": f"{id_val}",
                                    "datasets": [key_dataset],
                                }
                            else:
                                deriv_dict_id[val_dataset]["datasets"].append(
                                    key_dataset
                                )
        except KeyError as error:
            L.error(
                f"KeyError: {error} is not found in the dataset configuration file."
            )
            exit(1)

        try:
            for dataset in config_content["HierarchyJson"].keys():
                for key_dataset, val_dataset in provenance_metadata[
                    "derivations"
                ].items():
                    if key_dataset == dataset:
                        if isinstance(val_dataset, list):
                            for val in val_dataset:
                                id_val = forge.format(
                                    "identifier", "ontologies", str(uuid4())
                                )
                                if val not in deriv_dict_id.keys():
                                    deriv_dict_id[val] = {
                                        "id": f"{id_val}",
                                        "datasets": [key_dataset],
                                    }
                                else:
                                    deriv_dict_id[val]["datasets"].append(key_dataset)
                        else:
                            if val_dataset not in deriv_dict_id.keys():
                                id_val = forge.format(
                                    "identifier", "ontologies", str(uuid4())
                                )
                                deriv_dict_id[val_dataset] = {
                                    "id": f"{id_val}",
                                    "datasets": [key_dataset],
                                }
                            else:
                                deriv_dict_id[val_dataset]["datasets"].append(
                                    key_dataset
                                )
                # Assign to every ontology resource an id with the right format
                for deriv_dataset, deriv_metadata in deriv_dict_id.items():
                    if deriv_dataset == dataset:
                        deriv_metadata["id"] = forge.format(
                            "identifier", "ontologies", str(uuid4())
                        )
        except KeyError as error:
            L.error(
                f"KeyError: {error} is not found in the dataset configuration file."
            )
            exit(1)

    # Mutual resource properties

    atlas_reference_system_id = (
        "https://bbp.epfl.ch/neurosciencegraph/data/"
        "allen_ccfv3_spatial_reference_system"
    )

    # Link to the spatial ref system
    isRegisteredIn = {
        "@type": ["BrainAtlasSpatialReferenceSystem", "AtlasSpatialReferenceSystem"],
        "@id": atlas_reference_system_id,
    }

    subject = {
        "@type": "Subject",
        "species": {
            "@id": "http://purl.obolibrary.org/obo/NCBITaxon_10090",
            "label": "Mus musculus",
        },
    }

    # Create contribution
    if isinstance(forge._store, DemoStore):
        contribution = []
    else:
        try:
            contribution, log_info = return_contribution(forge)
            L.info("\n".join(log_info))
        except Exception as e:
            L.error(f"{e}")
            exit(1)

    # Config constants
    default_sampling_period = 30
    default_sampling_time_unit = "ms"
    spatial_unit = "µm"

    # Resource payload properties values corresponding to the different possible Atlas
    # volumetric datasets
    resource_type = "VolumetricDataLayer"

    description_ccfv2 = (
        f"original Allen ccfv2 annotation volume at {voxels_resolution} microns"
    )
    description_ccfv3 = (
        f"original Allen ccfv3 annotation volume at {voxels_resolution} microns"
    )
    description_hybrid = (
        f"Hybrid annotation volume from ccfv2 and ccfv3 at {voxels_resolution} "
        "microns"
    )
    description_split = "with the isocortex layer 2 and 3 split"
    description_ccfv3_split = f"{description_ccfv3} {description_split}"
    description_hybrid_split = f"{description_hybrid} {description_split}"

    description_dirvectors_ccfv3 = (
        f"3D unit vectors defined over the Original Allen ccfv3 annotation volume "
        f"(spatial resolution of {voxels_resolution} µm) and representing the neuron "
        "axone-to-dendrites orientation to voxels from the top regions of the "
        "Isocortex."
    )

    description_orientation = "Quaternions field (w,x,y,z) defined over the"
    description_orientation_end = (
        f"(spatial resolution of {voxels_resolution} µm) and representing the neuron "
        "axone-to-dendrites orientation to voxels from the Isocortex region."
    )
    description_orientation_ccfv3 = (
        f"{description_orientation} Original Allen ccfv3 annotation volume "
        f"{description_orientation_end}"
    )
    description_orientation_hybrid = (
        f"{description_orientation} CCF v2-v3 Hybrid annotation volume "
        f"{description_orientation_end}"
    )

    description_PH = (
        "The layers are ordered with respect to depth, which means that the layer "
        "which is the closest from the skull is the first layer (upper layer) and the "
        "deepest one is the last (lower layer)."
    )
    description_PH_ccfv3_split = (
        "Placement hints (cortical distance of voxels to layer boundaries) of the "
        f"Isocortex Layer XX of the {description_ccfv3_split}. {description_PH}"
    )
    # description_PH_hybrid_split = (
    #     "Placement hints (cortical distance of voxels to layer boundaries) of the "
    #     f"Isocortex Layer XX of the {description_hybrid_split}. {description_PH}"
    # )

    #  "@type": ["VolumetricDataLayer", "BrainParcellationDataLayer"],

    # derivation_descriptions = {
    #     "brain_parcellation_ccfv2": (
    #         "The original AIBS ccfv2 (2011) has a finer granularity than the "
    #         "original AIBS ccfv3 in term of leaf nodes"
    #         ),
    #     "brain_parcellation_ccfv3": (
    #         "The original AIBS ccfv3 (2017) has smoother region borders, without "
    #         "jaggies, compared to the original AIBS ccfv2."
    #         ),
    #     "annotation_hybrid_l23split": (
    #         "The separation between layer 2 and layer 3 was performed on the CCF "
    #         "v2-v3 hybrid volume."
    #         ),
    #     "annotation_ccfv3_l23split": (
    #         "The separation between layer 2 and layer 3 was performed on the "
    #         "original AIBS ccfv3 volume."
    #         ),
    # }

    derivation_ccfv2v3 = [
        {
            "@type": "Derivation",
            "description": (
                "The original AIBS ccfv3 (2017) has smoother region borders, without "
                "jaggies, compared to the original AIBS ccfv2."
            ),
            "entity": {
                "@id": "brain_parcellation_ccfv3",
                "@type": "Dataset",
            },
        },
        {
            "@type": "Derivation",
            "description": (
                "The original AIBS ccfv2 (2011) has a finer granularity than the "
                "original AIBS ccfv3 in term of leaf nodes"
            ),
            "entity": {
                "@id": "brain_parcellation_ccfv2",
                "@type": "Dataset",
            },
        },
    ]
    # "@type": ["VolumetricDataLayer", "BrainParcellationDataLayer"],
    derivation_hybrid = {
        "@type": "Derivation",
        "entity": {
            "@id": "annotation_v2v3_hybrid",
            "@type": "Dataset",
        },
    }
    derivation_hybrid_split = copy.deepcopy(derivation_hybrid)
    derivation_hybrid_split["description"] = (
        "The separation between layer 2 and layer 3 was performed on the CCF v2-v3 "
        "hybrid volume."
    )
    derivation_hybrid_split["entity"]["@id"] = "annotation_hybrid_l23split"

    # derivation_ccfv3 = {
    #     "@type": "Derivation",
    #     "entity": {
    #         "@id": "brain_parcellation_ccfv3",
    #         "@type": "Dataset",
    #     },
    # }
    # derivation_ccfv3_split = copy.deepcopy(derivation_hybrid)
    # derivation_ccfv3_split["description"] = (
    #     "The separation between layer 2 and layer 3 was performed on the original "
    #     "AIBS ccfv3 volume."
    # )
    # derivation_ccfv3_split["entity"]["@id"] = "annotation_ccfv3_l23split"

    derivation_correctednissl = {
        "@type": "Derivation",
        "entity": {
            "@id": "nissl_corrected_volume",  # “name_derivation”
            "@type": "Dataset",
        },
    }
    deriv_celldensities_template = {
        "@type": "Derivation",
        "entity": {
            "@id": "",
            "@type": "Dataset",
        },
    }

    atlasrelease_ccfv2_id = (
        "https://bbp.epfl.ch/neurosciencegraph/data/"
        "dd114f81-ba1f-47b1-8900-e497597f06ac"
    )

    atlasrelease_ccfv3_id = (
        "https://bbp.epfl.ch/neurosciencegraph/data/"
        "831a626a-c0ae-4691-8ce8-cfb7491345d9"
    )

    # Dictionary containing the possible volumetric dataset to push
    refined_inhib = "inhibitory_neuron_densities_preserveprop_ccfv2_correctednissl"
    volumetric_data = {
        "parcellations": {
            f"{volumes['annotation_hybrid']}": [
                "BrainParcellationDataLayer",  # subtype
                f"{description_hybrid}. The version "
                "replaces the leaf regions in ccfv3 with the leaf region of "
                "ccfv2, which have additional levels of hierarchy.",  # description
                derivation_ccfv2v3,  # derivation
                "atlasrelease_ccfv2v3",  # atlasrelease
                "parcellationId",  # datasamplemodality
            ],
            f"{volumes['annotation_hybrid_l23split']}": [
                "BrainParcellationDataLayer",
                description_hybrid_split,
                derivation_hybrid,
                "atlasrelease_hybridsplit",
                "parcellationId",
            ],
            f"{volumes['annotation_ccfv3_l23split']}": [
                "BrainParcellationDataLayer",
                description_ccfv3_split,
                None,
                "atlasrelease_ccfv3split",
                "parcellationId",
            ],
        },
        "cell_orientations": {
            f"{volumes['direction_vectors_isocortex_ccfv3']}": [
                "CellOrientationField",
                description_dirvectors_ccfv3,
                None,
                "atlasrelease_ccfv3",
                "eulerAngle",
            ],
            f"{volumes['cell_orientations_ccfv3']}": [
                "CellOrientationField",
                description_orientation_ccfv3,
                None,
                "atlasrelease_ccfv3",
                "quaternion",
            ],
            f"{volumes['cell_orientations_hybrid']}": [
                "CellOrientationField",
                description_orientation_hybrid,
                derivation_hybrid_split,
                "atlasrelease_ccfv2v3",
                "quaternion",
            ],
        },
        "placement_hints": {
            # f"{volumes['placement_hints_hybrid_l23split']}": [
            #     description_PH_hybrid_split,
            #     derivation_hybrid_split,
            #     "atlasrelease_hybridsplit",
            #     "distance",
            # ],
            f"{volumes['placement_hints_ccfv3_l23split']}": [
                "PlacementHintsDataLayer",
                description_PH_ccfv3_split,
                None,
                "atlasrelease_ccfv3split",
                "distance",
            ]
        },
        "volume_mask": {
            f"{volumes['brain_region_mask_ccfv3_l23split']}": [
                "BrainParcellationMask",
                description_ccfv3_split,
                None,
                "atlasrelease_ccfv3split",
                "parcellationId",
            ]
        },
        "cell_densities": {
            f"{volumes['cell_densities_hybrid']}": [
                "CellDensityDataLayer",
                description_hybrid,
                None,
                "atlasrelease_hybridsplit",
                "quantity",
            ],
            f"{volumes['neuron_densities_hybrid']}": [
                "CellDensityDataLayer",
                description_hybrid,
                None,
                "atlasrelease_hybridsplit",
                "quantity",
            ],
            f"{volumes['overall_cell_density_ccfv2_correctednissl']}": [
                "CellDensityDataLayer",
                f"{description_ccfv2}. It has been generated using the corrected nissl "
                "volume",
                derivation_correctednissl,
                "atlasrelease_ccfv2",
                "quantity",
            ],
            f"{volumes['cell_densities_ccfv2_correctednissl']}": [
                "CellDensityDataLayer",
                f"{description_ccfv2}. It has been generated using the corrected nissl "
                "volume",
                f"{volumes['overall_cell_density_ccfv2_correctednissl']}",
                "atlasrelease_ccfv2",
                "quantity",
            ],
            f"{volumes['neuron_densities_ccfv2_correctednissl']}": [
                "CellDensityDataLayer",
                f"{description_ccfv2}. It has been generated using the corrected nissl "
                "volume",
                (
                    f"{volumes['cell_densities_ccfv2_correctednissl']}",
                    "neuron_density.nrrd",
                ),
                "atlasrelease_ccfv2",
                "quantity",
            ],
            f"{volumes['inhibitory_neuron_densities_linprog_ccfv2_correctednissl']}": [
                "CellDensityDataLayer",
                f"{description_ccfv2}. It has been generated with the corrected nissl "
                "volume and using the algorithm linprog",
                (
                    f"{volumes['cell_densities_ccfv2_correctednissl']}",
                    "neuron_density.nrrd",
                ),
                "atlasrelease_ccfv2",
                "quantity",
            ],
            f"{volumes[refined_inhib]}": [
                "CellDensityDataLayer",
                f"{description_ccfv2}. It has been generated with the corrected nissl "
                "volume and using the algorithm keep-proportions",
                (
                    f"{volumes['cell_densities_ccfv2_correctednissl']}",
                    "neuron_density.nrrd",
                ),
                "atlasrelease_ccfv2",
                "quantity",
            ],
            f"{volumes['mtypes_densities_profile_ccfv2_correctednissl']}": [
                "CellDensityDataLayer",
                f"{description_ccfv2}. It has been generated from density profiles and "
                "using the corrected nissl volume",
                None,
                "atlasrelease_ccfv2",
                "quantity",
            ],
            f"{volumes['mtypes_densities_probability_map_ccfv2_correctednissl']}": [
                "CellDensityDataLayer",
                f"{description_ccfv2}. It has been generated from a probability mapping"
                " and using the corrected nissl volume",
                None,
                "atlasrelease_ccfv2",
                "quantity",
            ],
        },
    }

    # Constructs the Resource properties payloads with the dictionary of properties
    ressources_dict = {
        "datasets": [],
        "activity": [],
        "atlasreleases": [],
        "hierarchy": [],
    }
    atlasrelease_dict = {"atlasrelease_choice": None, "hierarchy": False}
    atlasrelease_choosen = []
    atlasRelease = {}
    dict_ids = {}
    generation = {}
    activity_resource = []
    for filepath in inputpath:
        file_found = False
        isFolder = False
        derivation = False
        isCellDensity = False
        isPH = False
        isRegionMask = False
        derivation_data_found = False
        dimension_name = False
        flat_tree = {}
        action_summary_file = False
        link_summary_content = {}
        different_atlasrelease = False
        atlasrelease_parcellation = None
        region_id = 997  # default: 997 --> root, whole brain
        region_name = "root"
        brainLocation = {
            "brainRegion": {"@id": f"mba:{region_id}", "label": f"{region_name}"},
            "atlasSpatialReferenceSystem": {
                "@type": [
                    "BrainAtlasSpatialReferenceSystem",
                    "AtlasSpatialReferenceSystem",
                ],
                "@id": atlas_reference_system_id,
            },
        }
        for dataset in volumetric_data["cell_densities"]:
            try:
                if os.path.samefile(filepath, dataset):
                    file_found = True
                    isCellDensity = True
                    if os.path.isdir(filepath):
                        isFolder = True
                        directory = filepath
                        files = os.listdir(directory)
                        pattern = "*_density.nrrd"
                        files_list = fnmatch.filter(files, pattern)
                        if not files_list:
                            L.error(
                                f"Error: '{filepath}' do not contain any cell density "
                                "volumetric files."
                            )
                            exit(1)
                        # this is going ot be the "name" of the resource
                        filepath = os.path.join(directory, files_list[0])
                        filename_noext = os.path.splitext(os.path.basename(filepath))[0]
                        file_extension = os.path.splitext(os.path.basename(filepath))[
                            1
                        ][1:]
                        file_split = filename_noext.split("_")
                        v = "mtypes_densities_probability_map_ccfv2_correctednissl"
                        try:
                            if os.path.samefile(
                                directory,
                                volumes[
                                    "mtypes_densities_profile_ccfv2_correctednissl"
                                ],
                            ) or os.path.samefile(
                                directory,
                                volumes[v],
                            ):
                                file_split.insert(0, "Mtype")
                        except FileNotFoundError:
                            pass
                        cell_density_file = " ".join(file_split)
                        atlasrelease_choice = volumetric_data["cell_densities"][
                            dataset
                        ][3]
                        filename_noext = cell_density_file
                        if atlasrelease_choice == "atlasrelease_ccfv2":
                            filename_noext = f"{filename_noext} Ccfv2 Corrected Nissl"
                        description = (
                            f"{cell_density_file[0].upper()}{cell_density_file[1:]} "
                            "volume for the "
                            f"{volumetric_data['cell_densities'][dataset][1]}."
                        )
                        voxel_type = "intensity"
                        resource_types = [
                            resource_type,
                            "GliaCellDensity",
                            "CellDensityDataLayer",
                        ]
                        # Derivation
                        if volumetric_data["cell_densities"][dataset][2]:
                            if not isinstance(
                                volumetric_data["cell_densities"][dataset][2], dict
                            ):
                                if isinstance(
                                    volumetric_data["cell_densities"][dataset][2], tuple
                                ):
                                    data_deriv = volumetric_data["cell_densities"][
                                        dataset
                                    ][2][0]
                                    subdata_deriv = volumetric_data["cell_densities"][
                                        dataset
                                    ][2][1]
                                    fulldata_deriv = f"{data_deriv}/{subdata_deriv}"
                                else:
                                    data_deriv = volumetric_data["cell_densities"][
                                        dataset
                                    ][2]
                                # Search for the file to derive from in the input files
                                # and if it is already a resource or not
                                for inputdata in inputpath:
                                    if os.path.samefile(
                                        data_deriv,
                                        inputdata,
                                    ):
                                        derivation_data_found = True
                                        for r in range(
                                            0, len(ressources_dict["datasets"])
                                        ):
                                            distrib = ressources_dict["datasets"][
                                                r
                                            ].distribution.args[0]
                                            if isinstance(
                                                volumetric_data["cell_densities"][
                                                    dataset
                                                ][2],
                                                tuple,
                                            ):
                                                data_deriv = fulldata_deriv
                                            # check if a previous resource is the one
                                            # from which it derives
                                            if os.path.samefile(data_deriv, distrib):
                                                # Check if the previous resource has
                                                # an id already
                                                try:
                                                    if ressources_dict["datasets"][
                                                        r
                                                    ].id:
                                                        pass
                                                except AttributeError:
                                                    dataset_id = forge.format(
                                                        "identifier",
                                                        "volumetricdatalayer",
                                                        str(uuid4()),
                                                    )
                                                    ressources_dict["datasets"][
                                                        r
                                                    ].id = f"{dataset_id}"
                                                derivation = copy.deepcopy(
                                                    deriv_celldensities_template
                                                )
                                                derivation["entity"][
                                                    "@id"
                                                ] = f"{dataset_id}"
                                                break
                                        # if the parent resource has not been created
                                        # yet
                                        if not derivation:
                                            dict_ids[f"{data_deriv}"] = forge.format(
                                                "identifier",
                                                "volumetricdatalayer",
                                                str(uuid4()),
                                            )
                                            derivation = copy.deepcopy(
                                                deriv_celldensities_template
                                            )
                                            derivation["entity"]["@id"] = dict_ids[
                                                f"{data_deriv}"
                                            ]
                                if not derivation_data_found:
                                    L.info(
                                        f"The file '{data_deriv}' whose "
                                        "resource corresponding to the file "
                                        f"'{dataset}' derivates is absent from the "
                                        "input dataset. The property 'derivation' "
                                        "from the latter resource will be thus left "
                                        "empty."
                                    )
                            else:
                                derivation = volumetric_data["cell_densities"][dataset][
                                    2
                                ]
                        dataSampleModality = volumetric_data["cell_densities"][dataset][
                            4
                        ]
                        break
                    else:
                        if filepath.endswith(".nrrd"):
                            file_found = True
                            # this is going ot be the "name" of the resource
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            file_split = filename_noext.split("_")[:3]
                            cell_density_file = " ".join(file_split)
                            atlasrelease_choice = volumetric_data["cell_densities"][
                                dataset
                            ][3]
                            filename_noext = cell_density_file
                            if atlasrelease_choice == "atlasrelease_ccfv2":
                                filename_noext = (
                                    f"{filename_noext} Ccfv2 Corrected Nissl"
                                )
                            description = (
                                f"{cell_density_file[0].upper()}"
                                f"{cell_density_file[1:]} volume for the "
                                f"{volumetric_data['cell_densities'][dataset][1]}."
                            )
                            voxel_type = "intensity"
                            resource_types = [
                                resource_type,
                                "GliaCellDensity",
                                "CellDensityDataLayer",
                            ]
                            # Derivation
                            if volumetric_data["cell_densities"][dataset][2]:
                                if not isinstance(
                                    volumetric_data["cell_densities"][dataset][2], dict
                                ):

                                    # Search for the file to derive from in the input
                                    # files and if it is already a resource or not
                                    if isinstance(
                                        volumetric_data["cell_densities"][dataset][2],
                                        tuple,
                                    ):
                                        data_deriv = volumetric_data["cell_densities"][
                                            dataset
                                        ][2][0]
                                        subdata_deriv = volumetric_data[
                                            "cell_densities"
                                        ][dataset][2][1]
                                        fulldata_deriv = f"{data_deriv}/{subdata_deriv}"
                                    else:
                                        data_deriv = volumetric_data["cell_densities"][
                                            dataset
                                        ][2]
                                    for inputdata in inputpath:
                                        if os.path.samefile(
                                            data_deriv,
                                            inputdata,
                                        ):
                                            derivation_data_found = True
                                            for r in range(
                                                0, len(ressources_dict["datasets"])
                                            ):
                                                distrib = ressources_dict["datasets"][
                                                    r
                                                ].distribution.args[0]
                                                if isinstance(
                                                    volumetric_data["cell_densities"][
                                                        dataset
                                                    ][2],
                                                    tuple,
                                                ):
                                                    data_deriv = fulldata_deriv
                                                if os.path.samefile(
                                                    data_deriv, distrib
                                                ):
                                                    try:
                                                        if ressources_dict["datasets"][
                                                            r
                                                        ].id:
                                                            pass
                                                    except AttributeError:
                                                        ressources_dict["datasets"][
                                                            r
                                                        ].id = forge.format(
                                                            "identifier",
                                                            "volumetricdatalayer",
                                                            str(uuid4()),
                                                        )
                                                    derivation = copy.deepcopy(
                                                        deriv_celldensities_template
                                                    )
                                                    derivation["entity"][
                                                        "@id"
                                                    ] = ressources_dict["datasets"][
                                                        r
                                                    ].id
                                                    break
                                            if not derivation:
                                                dict_ids[
                                                    f"{data_deriv}"
                                                ] = forge.format(
                                                    "identifier",
                                                    "volumetricdatalayer",
                                                    str(uuid4()),
                                                )
                                                derivation = copy.deepcopy(
                                                    deriv_celldensities_template
                                                )
                                                derivation["entity"]["@id"] = dict_ids[
                                                    f"{data_deriv}"
                                                ]
                                    if not derivation_data_found:
                                        L.info(
                                            f"The file '{data_deriv}' whose "
                                            "resource corresponding to the file "
                                            f"'{dataset}' derivates is absent from the "
                                            "input dataset. The property 'derivation' "
                                            "from the latter resource will be thus "
                                            "left empty."
                                        )
                                else:
                                    derivation = volumetric_data["cell_densities"][
                                        dataset
                                    ][2]
                            dataSampleModality = volumetric_data["cell_densities"][
                                dataset
                            ][4]
                            break
                        else:
                            L.error(
                                f"Error: parcellation dataset '{filepath}' is not a "
                                "volumetric .nrrd file"
                            )
                            exit(1)
            except FileNotFoundError:
                pass

        # Jump it if a file has been recognized
        if not file_found:
            for dataset in volumetric_data["parcellations"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        if filepath.endswith(".nrrd"):
                            file_found = True
                            voxel_type = "label"
                            resource_types = [
                                resource_type,
                                "BrainParcellationDataLayer",
                            ]
                            description = (
                                f"{volumetric_data['parcellations'][dataset][1]}"
                            )
                            derivation = volumetric_data["parcellations"][dataset][2]
                            atlasrelease_choice = volumetric_data["parcellations"][
                                dataset
                            ][3]
                            dataSampleModality = volumetric_data["parcellations"][
                                dataset
                            ][4]
                            # this is going ot be the "name" of the resource
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            break
                        else:
                            L.error(
                                f"Error: parcellation dataset '{filepath}' is not a "
                                "volumetric .nrrd file"
                            )
                            exit(1)
                except FileNotFoundError:
                    pass

        if not file_found:
            for dataset in volumetric_data["cell_orientations"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        if filepath.endswith(".nrrd"):
                            file_found = True
                            voxel_type = "vector"
                            resource_types = [resource_type, "CellOrientationField"]
                            description = (
                                f"{volumetric_data['cell_orientations'][dataset][1]}"
                            )
                            derivation = volumetric_data["cell_orientations"][dataset][
                                2
                            ]
                            atlasrelease_choice = volumetric_data["cell_orientations"][
                                dataset
                            ][3]
                            dataSampleModality = volumetric_data["cell_orientations"][
                                dataset
                            ][4]
                            if dataSampleModality == "quaternion":
                                dimension_name = "quaternion"
                            # this is going ot be the "name" of the resource
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            break
                        else:
                            L.error(
                                f"Error: parcellation dataset '{filepath}' is not a "
                                "volumetric .nrrd file"
                            )
                            exit(1)
                except FileNotFoundError:
                    pass

        if not file_found:
            for dataset in volumetric_data["placement_hints"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        file_found = True
                        isPH = True
                        if os.path.isdir(filepath):
                            isFolder = True
                            directory = filepath
                            files = os.listdir(directory)
                            pattern = "*layer*"
                            files_list = fnmatch.filter(files, pattern)
                            files_list = sorted(files_list)
                            if len(files_list) != 6:
                                L.error(
                                    f"Error: '{filepath}' do not contain 6 placement "
                                    "hints volumetric files."
                                )
                                exit(1)
                            pattern = "*y.nrrd"
                            ylayer = fnmatch.filter(files, pattern)
                            if not ylayer:
                                L.error(
                                    f"Error: '{filepath}' do not contain the file "
                                    "[PH]y.nrrd volumetric files."
                                )
                                exit(1)
                            pattern = "*problematic_voxel_mask.nrrd"
                            mask_error = fnmatch.filter(files, pattern)
                            if not mask_error:
                                L.error(
                                    f"Error: '{filepath}' do not contain the "
                                    "problematic_voxel_mask file."
                                )
                                exit(1)
                            pattern = "*report.json"
                            report_error = fnmatch.filter(files, pattern)
                            if not report_error:
                                L.error(
                                    f"Error: '{filepath}' do not contain the report "
                                    "json file"
                                )
                                exit(1)
                            files_list.extend(ylayer)
                            files_list.extend(mask_error)
                            files_list.extend(report_error)
                            # this is going ot be the "name" of the resource
                            filepath = os.path.join(directory, files_list[0])
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            voxel_type = "vector"
                            resource_types = [
                                resource_type,
                                "PlacementHintsDataLayer",
                            ]
                            layer_number = re.findall(
                                r"\d+", filename_noext.replace("_", " ").title()
                            )
                            description = volumetric_data["placement_hints"][dataset][1]
                            description = description.replace("XX", layer_number[0])
                            atlasrelease_choice = volumetric_data["placement_hints"][
                                dataset
                            ][3]
                            dataSampleModality = volumetric_data["placement_hints"][
                                dataset
                            ][4]
                            suffixe = "CCFv3 L23 Split"
                            annotation_description = description_ccfv3_split
                            # if (
                            #     dataset
                            #     == f"{volumes['placement_hints_hybrid_l23split']}"
                            # ):
                            #     suffixe = "CCF v2-v3 Hybrid L23 Split"
                            #     annotation_description = description_hybrid_split
                            break
                        else:
                            L.error(
                                f"Error: placement hints dataset '{filepath}' is not a "
                                "folder containing placement hints volume and error "
                                "reports."
                            )
                            exit(1)

                except FileNotFoundError:
                    pass

        if not file_found:
            for dataset in volumetric_data["volume_mask"]:
                try:
                    if os.path.samefile(filepath, dataset):
                        isRegionMask = True
                        file_found = True
                        if os.path.isdir(filepath):
                            isFolder = True
                            directory = filepath
                            files = os.listdir(directory)
                            pattern = "*.nrrd"
                            files_list = fnmatch.filter(files, pattern)
                            if not files_list:
                                L.error(
                                    f"Error: '{filepath}' do not contain any mask "
                                    "volumetric files."
                                )
                                exit(1)
                            # this is going ot be the "name" of the resource
                            filepath = os.path.join(directory, files_list[0])
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            voxel_type = "label"
                            resource_types = [
                                resource_type,
                                "BrainParcellationMask",
                            ]
                            try:
                                region_id = int(filename_noext)
                            except ValueError as error:
                                L.error(
                                    f"ValueError in '{filepath}' file name. {error}. "
                                    "The mask file names have to be integer "
                                    "representing their region"
                                )
                                exit(1)
                            try:
                                region_name, hierarchy_tree = get_brain_region_prop(
                                    region_id, ["name"], input_hierarchy, flat_tree
                                )
                                region_name = region_name["name"]
                                flat_tree = hierarchy_tree
                            except KeyError as e:
                                L.error(f"KeyError: {e}")
                                exit(1)
                            except ValueError as e:
                                L.error(f"ValueError: {e}")
                                exit(1)
                            L.info(
                                f"Creating the Mask payload for region {region_id}..."
                            )
                            description = (
                                f"Binary mask volume - {region_name.title()} (ID: "
                                f"{region_id}) - for the "
                                f"{volumetric_data['volume_mask'][dataset][1]}."
                            )
                            brainLocation = {
                                "brainRegion": {
                                    "@id": f"mba:{region_id}",
                                    "label": region_name,
                                },
                                "atlasSpatialReferenceSystem": {
                                    "@type": [
                                        "BrainAtlasSpatialReferenceSystem",
                                        "AtlasSpatialReferenceSystem",
                                    ],
                                    "@id": atlas_reference_system_id,
                                },
                            }
                            derivation = volumetric_data["volume_mask"][dataset][2]
                            atlasrelease_choice = volumetric_data["volume_mask"][
                                dataset
                            ][3]
                            dataSampleModality = volumetric_data["volume_mask"][
                                dataset
                            ][4]
                            # this is going ot be the "name" of the resource
                            filename_noext = os.path.splitext(
                                os.path.basename(filepath)
                            )[0]
                            file_extension = os.path.splitext(
                                os.path.basename(filepath)
                            )[1][1:]
                            filename_noext = f"{region_name.title()} Mask"
                            if atlasrelease_choice == "atlasrelease_ccfv3split":
                                filename_noext = f"{filename_noext} Ccfv2 L23split"
                            if link_regions_path:
                                try:
                                    with open(
                                        link_regions_path, "r+"
                                    ) as link_summary_file:
                                        link_summary_file = open(
                                            link_regions_path, "r+"
                                        )
                                        link_summary_file.seek(0)
                                        link_summary_content = json.loads(
                                            link_summary_file.read()
                                        )
                                    action_summary_file = "append"
                                except json.decoder.JSONDecodeError:
                                    action_summary_file = "write"
                                except FileNotFoundError:
                                    action_summary_file = "write"
                            break
                        else:
                            L.error(
                                f"Error: volumetric mask dataset '{filepath}' is not a "
                                "folder containing binary mask volume."
                            )
                            exit(1)

                except FileNotFoundError:
                    pass

        # If still no file found at this step then raise error
        if not file_found:
            L.error(
                f"Error: '{filepath}' does not correspond to one of the datasets "
                "defined in the VolumetricFile section of the 'generated dataset' "
                "configuration file"
            )
            exit(1)

        # Useful variable for derivation and atlasrelease parcellation/ontology linking
        if isFolder:
            dataset_name = os.path.basename(directory)
        else:
            dataset_name = filename_noext

        # Create the Derivation link
        if not derivation:
            if not isCellDensity:
                derivation = []
                for deriv_key, deriv_value in deriv_dict_id.items():
                    if dataset_name in deriv_value["datasets"]:
                        deriv_type = []
                        for volumetric_type, content in volumetric_data.items():
                            try:
                                deriv_type = content[f"{volumes[deriv_key]}"][0]
                                if deriv_type not in resource_types:
                                    # deriv_type = ["Dataset", deriv_type]
                                    deriv_type = "Dataset"
                                else:
                                    deriv_type = "Dataset"
                            except KeyError:
                                pass
                        # if the derivation is not a known volumetric dataset then
                        # it is an ontology
                        if not deriv_type:
                            deriv_type = ["Entity", "ParcellationOntology"]
                        deriv = {
                            "@type": "Derivation",
                            "entity": {
                                "@id": deriv_value["id"],
                                "@type": deriv_type,
                            },
                        }
                        derivation.append(deriv)
                # If only 1 item no need for it to be a list
                if len(derivation) == 1:
                    derivation = derivation[0]
                if not derivation:
                    # Default derivation for everything = 'input_dataset_used'
                    try:
                        for metadata in provenance_metadata[
                            "input_dataset_used"
                        ].values():
                            # Use types different from the Resource type
                            if metadata["type"] == "ParcellationOntology":
                                deriv_type = ["Entity", metadata["type"]]
                            else:
                                deriv_type = metadata["type"]
                                if deriv_type not in resource_types:
                                    # deriv_type = ["Dataset", deriv_type]
                                    deriv_type = "Dataset"
                                else:
                                    deriv_type = "Dataset"
                            deriv = {
                                "@type": "Derivation",
                                "entity": {
                                    "@id": metadata["id"],
                                    "@type": deriv_type,
                                },
                            }
                            if len(provenance_metadata["input_dataset_used"]) == 1:
                                derivation = deriv
                            else:
                                derivation.append(deriv)
                    except Exception as error:
                        L.error(f"Error: {error}.")
                        exit(1)
                    except RetrievalError as error:
                        L.error(
                            f"Error when trying to retrieve the Resources contained in "
                            f" 'input_dataset_used'. {error}."
                        )
                        exit(1)

        # Parsing the header of the NRRD file
        header = None
        try:
            header = nrrd.read_header(filepath)
        except nrrd.errors.NRRDError as e:
            L.error(f"NrrdError: {e}")
            L.info("Aborting pushing process.")  # setLevel(logging.INFO)
            exit(1)

        config = {
            "file_extension": file_extension,
            "sampling_space_unit": spatial_unit,
            "sampling_period": default_sampling_period,
            "sampling_time_unit": default_sampling_time_unit,
        }

        # Create and add the AtlasRelease link
        if atlasrelease_choice == "atlasrelease_ccfv2":
            atlasRelease = {
                "@id": f"{atlasrelease_ccfv2_id}",
                "@type": ["AtlasRelease", "BrainAtlasRelease", "Entity"],
            }
        elif atlasrelease_choice == "atlasrelease_ccfv3":
            atlasRelease = {
                "@id": f"{atlasrelease_ccfv3_id}",
                "@type": ["AtlasRelease", "BrainAtlasRelease", "Entity"],
            }
        # elif atlasrelease_choice == "atlasrelease_ccfv3split":
        #     atlasRelease = {
        #         "@id": "https://bbp.epfl.ch/neurosciencegraph/data/brainatlasrelease/"
        #         "5149d239-8b4d-43bb-97b7-8841a12d85c4",
        #         "@type": ["AtlasRelease", "BrainAtlasRelease", "Entity"],
        #     }

        # For a new atlas release creation verify first that the right parcellation
        # volume and hierarchy file have been provided
        elif not isinstance(forge._store, DemoStore):
            # Check that the same atlasrelease is not treated again
            if not atlasrelease_dict["atlasrelease_choice"] or (
                atlasrelease_choice not in atlasrelease_choosen
            ):
                different_atlasrelease = True
                atlasrelease_choosen.append(atlasrelease_choice)
                if atlasrelease_choice == "atlasrelease_ccfv2v3":
                    hierarchy_choice = config_content["HierarchyJson"]["hierarchy"]
                    atlasrelease_parcellation = "annotation_ccfv3_l23split"
                elif atlasrelease_choice == "atlasrelease_hybridsplit":
                    hierarchy_choice = config_content["HierarchyJson"][
                        "hierarchy_l23split"
                    ]
                    atlasrelease_parcellation = "annotation_hybrid_l23split"
                elif atlasrelease_choice == "atlasrelease_ccfv3split":
                    hierarchy_choice = config_content["HierarchyJson"][
                        "hierarchy_l23split"
                    ]
                    atlasrelease_parcellation = "annotation_ccfv3_l23split"
                try:
                    if os.path.samefile(
                        hierarchy_choice,
                        input_hierarchy,
                    ):
                        atlasrelease_hierarchy = input_hierarchy
                except FileNotFoundError as error:
                    L.error(
                        f"FileNotFoundError: {error}.  The atlasrelease corresponding "
                        f"to some of the input datasets is '{atlasrelease_choice}'. "
                        f"Thus, the hierarchy file '{hierarchy_choice}' and the "
                        f"parcellation file '{atlasrelease_parcellation}' need to be "
                        "respectively provided by the arguments '--input-hierarchy' "
                        "and '--dataset-path'"
                    )
                    exit(1)

                atlasrelease_dict["atlasrelease_choice"] = atlasrelease_choice
                try:
                    atlasrelease_dict = return_atlasrelease(
                        forge,
                        config_content,
                        atlasrelease_id,
                        atlasrelease_dict,
                        atlasrelease_parcellation,
                        atlasrelease_hierarchy,
                        atlas_reference_system_id,
                        subject,
                    )
                except Exception as e:
                    L.error(f"Exception: {e}")
                    exit(1)
                except FileNotFoundError as e:
                    L.error(f"FileNotFoundError: {e}")
                    exit(1)
                # if atlasrelease are ccfv2 and ccfv3
                if isinstance(atlasrelease_dict["atlas_release"], list):
                    atlasRelease = [
                        {
                            "@id": atlasrelease_dict["atlas_release"][0].id,
                            "@type": atlasrelease_dict["atlas_release"][0].type,
                        },
                        {
                            "@id": atlasrelease_dict["atlas_release"][1].id,
                            "@type": atlasrelease_dict["atlas_release"][1].type,
                        },
                    ]
                    if atlasrelease_dict["create_new"]:
                        atlasrelease_dict["atlas_release"][
                            0
                        ].contribution = contribution
                        atlasrelease_dict["atlas_release"][
                            1
                        ].contribution = contribution
                        ressources_dict["atlasreleases"].append(
                            atlasrelease_dict["atlas_release"][0]
                        )
                        ressources_dict["atlasreleases"].append(
                            atlasrelease_dict["atlas_release"][1]
                        )
                else:
                    atlasRelease = {
                        "@id": atlasrelease_dict["atlas_release"].id,
                        "@type": atlasrelease_dict["atlas_release"].type,
                    }
                # Annotate ontology id and derivation + link the atlasrelease to the
                # ontology
                hierarchy_name = os.path.splitext(
                    os.path.basename(atlasrelease_hierarchy)
                )[0]
                if hierarchy_name in deriv_dict_id.keys():
                    atlasrelease_dict["hierarchy"].id = deriv_dict_id[hierarchy_name][
                        "id"
                    ]

                hierarchy_deriv = []
                for deriv_key, deriv_value in deriv_dict_id.items():
                    if hierarchy_name in deriv_value["datasets"]:
                        deriv_type = []
                        for volumetric_type, content in volumetric_data.items():
                            try:
                                deriv_type = content[f"{volumes[deriv_key]}"][0]
                                if deriv_type not in resource_types:
                                    deriv_type = ["Dataset", deriv_type]
                                else:
                                    deriv_type = "Dataset"
                            except KeyError:
                                pass
                        # if the derivation is not a known volumetric dataset then
                        # it is an ontology
                        if not deriv_type:
                            deriv_type = ["Entity", "ParcellationOntology"]
                        deriv = {
                            "@type": "Derivation",
                            "entity": {
                                "@id": deriv_value["id"],
                                "@type": deriv_type,
                            },
                        }
                        hierarchy_deriv.append(deriv)
                # If only 1 item no need for it to be a list
                if len(hierarchy_deriv) == 1:
                    hierarchy_deriv = derivation[0]

                atlasrelease_dict["atlas_release"].parcellationOntology = {
                    "@id": atlasrelease_dict["hierarchy"].id,
                    "@type": ["Entity", "ParcellationOntology", "Ontology"],
                }
                atlasrelease_dict["hierarchy"].contribution = contribution
                ressources_dict["hierarchy"].append(atlasrelease_dict["hierarchy"])

        if provenance_metadata:
            try:
                activity_resource = return_activity_payload(forge, provenance_metadata)
            except Exception as e:
                L.error(f"{e}")
                exit(1)

            # # if activity has been created and not fetched from Nexus
            # if activity_resource._store_metadata:
            #     if hasattr(activity_resource, "startedAtTime"):
            #         activity_resource.startedAtTime = forge.from_json(
            #             {
            #                 "type": activity_resource.startedAtTime.type,
            #                 "@value": activity_resource.startedAtTime.value,
            #             }
            #         )

            generation = {
                "@type": "Generation",
                "activity": {
                    "@id": activity_resource.id,
                    "@type": activity_resource.type,
                },
            }

        name = filename_noext.replace("_", " ").title()

        if isPH:
            name = f"{name} {suffixe}"
            # brainLocation["layer"] = f"{layer_number[0]}"

        # We create a 1st payload that will be recycled in case of multiple files to
        # push
        content_type = f"application/{file_extension}"
        distribution_file = forge.attach(filepath, content_type)

        resource_types.append("Dataset")
        nrrd_resource = Resource(
            type=resource_types,
            name=name,
            distribution=distribution_file,
            description=description,
            isRegisteredIn=isRegisteredIn,
            brainLocation=brainLocation,
            atlasRelease=atlasRelease,
            dataSampleModality=dataSampleModality,
            subject=subject,
            contribution=contribution,
        )

        nrrd_resource = add_nrrd_props(nrrd_resource, header, config, voxel_type)

        if derivation:
            nrrd_resource.derivation = derivation

        if dataset in dict_ids:
            nrrd_resource.id = dict_ids[dataset]
        elif dataset_name in deriv_dict_id.keys():
            nrrd_resource.id = deriv_dict_id[dataset_name]["id"]

        if dimension_name:
            nrrd_resource.dimension[0]["name"] = dimension_name

        if generation:
            nrrd_resource.generation = generation

        if action_summary_file:
            atlasrelease_link = {"atlasRelease": {"@id": atlasRelease["@id"]}}
            try:
                if nrrd_resource.id:
                    mask_id = nrrd_resource.id
                else:
                    mask_id = forge.format(
                        "identifier", "volumetricdatalayer", str(uuid4())
                    )
                    nrrd_resource.id = mask_id
            except AttributeError:
                mask_id = forge.format(
                    "identifier", "volumetricdatalayer", str(uuid4())
                )
                nrrd_resource.id = mask_id
            mask_link = {"mask": {"@id": mask_id}}
            if action_summary_file == "append":
                try:
                    if (
                        "atlasRelease"
                        not in link_summary_content[f"{region_id}"].keys()
                    ):
                        link_summary_content[f"{region_id}"].update(atlasrelease_link)
                    else:
                        link_summary_content[f"{region_id}"] = atlasrelease_link
                    if "mask" not in link_summary_content[f"{region_id}"].keys():
                        link_summary_content[f"{region_id}"].update(mask_link)
                    else:
                        link_summary_content[f"{region_id}"] = mask_link
                except KeyError as error:
                    L.error(
                        f"KeyError: The region whose region id is {error} cannot be "
                        "found in the input link region json file"
                    )
                    exit(1)
            else:
                region_summary = {
                    f"{region_id}": {
                        "atlasRelease": {"@id": atlasRelease["@id"]},
                        "mask": {"@id": mask_id},
                    }
                }
                link_summary_content.update(region_summary)

        # Link the atlasrelease to its parcellation and supplement its linked ontology
        # Resource
        if not isinstance(forge._store, DemoStore):
            try:
                if atlasrelease_parcellation and os.path.samefile(
                    volumes[atlasrelease_parcellation], filepath
                ):
                    atlasrelease_dict["atlas_release"].parcellationVolume = {
                        "@id": nrrd_resource.id,
                        "@type": ["Dataset", "BrainParcellationDataLayer"],
                    }
                    atlasrelease_dict["atlas_release"].contribution = contribution
            except FileNotFoundError:
                pass

        if different_atlasrelease:
            if generation:
                atlasrelease_dict["hierarchy"].generation = generation
                atlasrelease_dict["atlas_release"].generation = generation
            ressources_dict["atlasreleases"].append(atlasrelease_dict["atlas_release"])

        ressources_dict["datasets"].append(nrrd_resource)

        # If the input is a folder containing several dataset to push
        if isFolder:
            for f in range(1, len(files_list)):  # start at the 2nd file
                filepath = os.path.join(directory, files_list[f])
                filename_noext = os.path.splitext(os.path.basename(filepath))[0]
                name = filename_noext.replace("_", " ").title()
                distribution_file = forge.attach(filepath, content_type)

                if isCellDensity:
                    file_split = filename_noext.split("_")
                    v = "mtypes_densities_probability_map_ccfv2_correctednissl"
                    try:
                        if os.path.samefile(
                            directory,
                            volumes["mtypes_densities_profile_ccfv2_correctednissl"],
                        ) or os.path.samefile(
                            directory,
                            volumes[v],
                        ):
                            file_split.insert(0, "Mtype")
                    except FileNotFoundError:
                        pass
                    cell_density_file = " ".join(file_split)
                    description = (
                        f"{cell_density_file[0].upper()}{cell_density_file[1:]} volume "
                        f"for the {volumetric_data['cell_densities'][dataset][1]}."
                    )
                    name = cell_density_file.title()
                    if atlasrelease_choice == "atlasrelease_ccfv2":
                        name = f"{name} Ccfv2 Corrected Nissl"

                if isPH:
                    # Do not create specific payload for the json report because it is
                    # included with the problematic mask
                    if f >= 8:
                        break
                    name = f"{name} {suffixe}"
                    if f <= 5:
                        layer_number = re.findall(r"\d+", files_list[f])
                        description = volumetric_data["placement_hints"][dataset][1]
                        description = description.replace("XX", layer_number[0])
                    # else:
                    #     try:
                    #         brainLocation.pop("layer")
                    #     except KeyError:
                    #         pass
                    if f == 6:
                        description = "Volume containing for each voxel its distance "
                        f"from the bottom of the {annotation_description} Isocortex. "
                        "The bottom being the deepest part of the Isocortex (highest "
                        "cortical depth)."
                    if f == 7:
                        description = (
                            f"3D mask volume of the {annotation_description}. It "
                            "includes Isocortex problematic voxels along with the "
                            "distance JSON report containing the proportion of "
                            "problematic voxels. Problematic voxels are highlighted "
                            "during the placement-hints computation and correspond to "
                            "voxels with at least one distance-related problem (i.e "
                            "those who do not intersect with the bottom or top mesh, "
                            "those with a distance gap greater than the maximum "
                            "thickness...)."
                        )
                        distribution_mask = forge.attach(filepath, content_type)
                        report_json = os.path.join(directory, files_list[8])
                        content_type = (
                            f"application/"
                            f"{os.path.splitext(os.path.basename(report_json))[1][1:]}"
                        )
                        distribution_json = forge.attach(report_json, content_type)
                        distribution_file = [distribution_mask, distribution_json]

                if isRegionMask:
                    try:
                        region_id = int(filename_noext)
                    except ValueError as error:
                        L.error(
                            f"ValueError in '{filepath}' file name. {error}. "
                            "The mask file names have to be integer "
                            "representing their region"
                        )
                        exit(1)
                    try:
                        region_name, hierarchy_tree = get_brain_region_prop(
                            region_id, ["name"], input_hierarchy, flat_tree
                        )
                        region_name = region_name["name"]
                    except KeyError as e:
                        L.error(f"KeyError: {e}")
                        exit(1)
                    L.info(f"Creating the Mask payload for region {region_id}...")
                    description = (
                        f"Binary mask volume - {region_name.title()} (ID: "
                        f"{region_id}) - for the "
                        f"{volumetric_data['volume_mask'][dataset][1]}."
                    )
                    name = f"{region_name.title()} Mask"
                    brainLocation = {
                        "brainRegion": {
                            "@id": f"mba:{region_id}",
                            "label": region_name,
                        },
                        "atlasSpatialReferenceSystem": {
                            "@type": [
                                "BrainAtlasSpatialReferenceSystem",
                                "AtlasSpatialReferenceSystem",
                            ],
                            "@id": atlas_reference_system_id,
                        },
                    }

                    if atlasrelease_choice == "atlasrelease_ccfv3split":
                        name = f"{name} Ccfv2 L23split"
                    if action_summary_file:
                        atlasrelease_link = {
                            "atlasRelease": {"@id": atlasRelease["@id"]}
                        }
                        if dataset_name in deriv_dict_id.keys():
                            mask_id = deriv_dict_id[dataset_name]["id"]
                        else:
                            mask_id = forge.format(
                                "identifier", "volumetricdatalayer", str(uuid4())
                            )
                        mask_link = {"mask": {"@id": mask_id}}
                        if action_summary_file == "append":
                            try:
                                if (
                                    "atlasRelease"
                                    not in link_summary_content[f"{region_id}"].keys()
                                ):
                                    link_summary_content[f"{region_id}"].update(
                                        atlasrelease_link
                                    )
                                else:
                                    link_summary_content[
                                        f"{region_id}"
                                    ] = atlasrelease_link
                                if (
                                    "mask"
                                    not in link_summary_content[f"{region_id}"].keys()
                                ):
                                    link_summary_content[f"{region_id}"].update(
                                        mask_link
                                    )
                                else:
                                    link_summary_content[f"{region_id}"] = mask_link
                            except KeyError as error:
                                L.error(
                                    "KeyError: The region whose region id is "
                                    f"'{error}' can not be found in the input link "
                                    "region json file"
                                )
                                exit(1)
                        else:
                            region_summary = {
                                f"{region_id}": {
                                    "atlasRelease": {"@id": atlasRelease["@id"]},
                                    "mask": {"@id": mask_id},
                                }
                            }
                            link_summary_content.update(region_summary)
                    if f == len(files_list) - 1:
                        link_summary_file = open(link_regions_path, "w")
                        link_summary_file.write(
                            json.dumps(
                                link_summary_content, ensure_ascii=False, indent=2
                            )
                        )
                        link_summary_file.close()

                # Use forge.reshape instead ?
                nrrd_resources = Resource(
                    type=nrrd_resource.type,
                    name=name,
                    distribution=distribution_file,
                    description=description,
                    contribution=nrrd_resource.contribution,
                    isRegisteredIn=nrrd_resource.isRegisteredIn,
                    brainLocation=brainLocation,
                    atlasRelease=nrrd_resource.atlasRelease,
                    componentEncoding=nrrd_resource.componentEncoding,
                    fileExtension=nrrd_resource.fileExtension,
                    dimension=nrrd_resource.dimension,
                    sampleType=nrrd_resource.sampleType,
                    worldMatrix=nrrd_resource.worldMatrix,
                    resolution=nrrd_resource.resolution,
                    bufferEncoding=nrrd_resource.bufferEncoding,
                    endianness=nrrd_resource.endianness,
                    dataSampleModality=nrrd_resource.dataSampleModality,
                    subject=nrrd_resource.subject,
                )

                if isPH:
                    if 5 < f < 8:
                        try:
                            header = nrrd.read_header(filepath)
                        except nrrd.errors.NRRDError as e:
                            L.error(f"NrrdError: {e}")
                            L.info("Aborting pushing process.")
                            exit(1)
                        config["file_extension"] = os.path.splitext(
                            os.path.basename(files_list[f])
                        )[1][1:]
                        voxel_type = "label"
                        nrrd_resources = add_nrrd_props(
                            nrrd_resources, header, config, voxel_type
                        )
                    if f >= 7:
                        nrrd_resources.dataSampleModality = "mask"
                        nrrd_resources.type = [
                            "VolumetricDataLayer",
                            "PlacementHintsDataReport",
                            "Dataset",
                        ]

                if derivation:
                    nrrd_resources.derivation = nrrd_resource.derivation

                if dataset_name in deriv_dict_id.keys():
                    nrrd_resource.id = deriv_dict_id[dataset_name]["id"]

                if generation:
                    nrrd_resources.generation = nrrd_resource.generation

                if action_summary_file:
                    nrrd_resources.id = mask_id

                ressources_dict["datasets"].append(nrrd_resources)

    ressources_dict["activity"] = activity_resource

    return ressources_dict


def add_nrrd_props(resource, nrrd_header, config, voxel_type):
    """
    Add to the resource all the fields expected for a VolumetricDataLayer/NdRaster
    that can be found in the NRRD header. A resource dictionary must exist and be
    provided (even if empty).

    Parameters:
        resource : Resource object defined by a properties payload linked to a file.
        nrrd_header : Dict containing the input file header fields  and their
        corresponding value.
        config : Dict containing the file extension and its sampling informations.
        voxel_type : String indicating the type of voxel contained in the volumetric
                     dataset.

    Returns:
        dataset : Resource object with all Nrrd properties added.
    """

    NRRD_TYPES_TO_NUMPY = {
        "signed char": "int8",
        "int8": "int8",
        "int8_t": "int8",
        "uchar": "uint8",
        "unsigned char": "uint8",
        "uint8": "uint8",
        "uint8_t": "uint8",
        "short": "int16",
        "short int": "int16",
        "signed short": "int16",
        "signed short int": "int16",
        "int16": "int16",
        "int16_t": "int16",
        "ushort": "int16",
        "unsigned short": "uint16",
        "unsigned short int": "uint16",
        "uint16": "uint16",
        "uint16_t": "uint16",
        "int": "int32",
        "signed int": "int32",
        "int32": "int32",
        "int32_t": "int32",
        "uint": "uint32",
        "unsigned int": "uint32",
        "uint32": "uint32",
        "uint32_t": "uint32",
        "longlong": "int64",
        "long long": "int64",
        "long long int": "int64",
        "signed long long": "int64",
        "signed long long int": "int64",
        "int64": "int64",
        "int64_t": "int64",
        "ulonglong": "uint64",
        "unsigned long long": "uint64",
        "unsigned long long int": "uint64",
        "uint64": "uint64",
        "uint64_t": "uint64",
        "float": "float32",
        "double": "float64",
    }

    space_origin = None
    if "space origin" in nrrd_header:
        space_origin = nrrd_header["space origin"].tolist()
    else:
        if nrrd_header["dimension"] == 2:
            space_origin = [0.0, 0.0]
        elif nrrd_header["dimension"] == 3:
            space_origin = [0.0, 0.0, 0.0]

    space_directions = None
    if "space directions" in nrrd_header:
        # replace the nan that pynrrd adds to None (just like in NRRD spec)
        space_directions = []
        for col in nrrd_header["space directions"].tolist():
            if np.isnan(col).any():
                space_directions.append(None)
            else:
                space_directions.append(col)

    # Here, 'space directions' being missing in the file, we hardcode an identity matrix
    # If we have 4 dimensions, we say
    else:
        if nrrd_header["dimension"] == 2:
            space_directions = [[1, 0], [0, 1]]
        elif nrrd_header["dimension"] == 3:
            space_directions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
        elif nrrd_header["dimension"] == 4:
            # the following is a very lousy way to determine if among the 4 dims,
            # or the first is components or the last is time...
            if nrrd_header["sizes"][0] < (np.mean(nrrd_header["sizes"] * 0.20)):
                space_directions = [None, [1, 0, 0], [0, 1, 0], [0, 0, 1]]  # component
            else:
                space_directions = [[1, 0, 0], [0, 1, 0], [0, 0, 1], None]  # time

        elif nrrd_header["dimension"] == 5:
            space_directions = [None, [1, 0, 0], [0, 1, 0], [0, 0, 1], None]

    resource.componentEncoding = NRRD_TYPES_TO_NUMPY[nrrd_header["type"]]
    # in case the nrrd file corresponds to a mask
    try:
        resource.endianness = nrrd_header["endian"]
    except KeyError:
        resource.endianness = "little"
    resource.bufferEncoding = nrrd_header["encoding"]
    resource.fileExtension = config["file_extension"]
    resource.dimension = []

    component_dim_index = -1
    passed_spatial_dim = False
    # for each dimension
    for i in range(0, nrrd_header["dimension"]):
        current_dim = {}
        current_dim["size"] = nrrd_header["sizes"][i].item()

        # this is a spatial dim
        if space_directions[i]:
            passed_spatial_dim = True
            current_dim["@type"] = "SpaceDimension"
            current_dim["unitCode"] = config["sampling_space_unit"]

        # this can be a component or a time dim
        else:
            # this is a time dim as it is located after space dim)
            if passed_spatial_dim:
                current_dim["@type"] = "TimeDimension"
                current_dim["samplingPeriod"] = config["sampling_period"]
                current_dim["unitCode"] = config["sampling_time_unit"]

            # this is a component dim as it is located before space dim
            else:
                # decide of the label
                component_dim_index = i
                current_dim["@type"] = "ComponentDimension"
                # current_dim["name"] = default_sample_type_multiple_components if
                # current_dim["size"] > 1 else default_sample_type_single_component
                try:
                    current_dim["name"] = get_voxel_type(
                        voxel_type, current_dim["size"]
                    )
                except ValueError as e:
                    L.error(f"ValueError: {e}")
                    exit(1)
                except KeyError as e:
                    L.error(f"KeyError: {e}")
                    exit(1)

        resource.dimension.append(current_dim)

    # repeating the name of the component dimension in the "sampleType" base level prop
    if component_dim_index >= 0:
        resource.sampleType = resource.dimension[component_dim_index]["name"]

    # As no component dim was mentioned in metadata, it means the component is of size 1
    else:
        # prepend a dimension component
        try:
            name = get_voxel_type(voxel_type, 1)
        except ValueError as e:
            L.error(f"ValueError: {e}")
            exit(1)
        component_dim = {"@type": "ComponentDimension", "size": 1, "name": name}
        resource.dimension.insert(0, component_dim)

        resource.sampleType = component_dim["name"]

    # creating the world matrix (column major)
    # 1. pynrrd creates a [nan, nan, nan] line for each 'space directions' that is
    # 'none' in the header.
    # We have to strip them off.
    worldMatrix = None
    r = []  # rotation mat
    o = space_origin
    for col in space_directions:
        if col is not None:
            r.append(col)

    # if 3D, we create a 4x4 homogeneous transformation matrix
    if len(r) == 3:
        worldMatrix = [
            r[0][0],
            r[0][1],
            r[0][2],
            0,
            r[1][0],
            r[1][1],
            r[1][2],
            0,
            r[2][0],
            r[2][1],
            r[2][2],
            0,
            o[0],
            o[1],
            o[2],
            1,
        ]

    # if 2D, we create a 3x3 homogeneous transformation matrix
    if len(r) == 2:
        worldMatrix = [r[0][0], r[0][1], 0, r[1][0], r[1][1], 0, o[0], o[1], 1]

    # nesting the matrix values into object with @value props
    for i in range(0, len(worldMatrix)):
        # worldMatrix[i] = {"@value": float(worldMatrix[i])}
        worldMatrix[i] = float(worldMatrix[i])

    resource.worldMatrix = worldMatrix

    resource.resolution = {"value": r[0][0], "unitCode": config["sampling_space_unit"]}

    return resource
